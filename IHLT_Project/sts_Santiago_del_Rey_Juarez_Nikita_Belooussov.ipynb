{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey Ju√°rez\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgling in c:\\users\\nbelo\\anaconda3\\envs\\ihlt\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: svgwrite in c:\\users\\nbelo\\anaconda3\\envs\\ihlt\\lib\\site-packages (from svgling) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import spacy\n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") #if this does not work run python -m spacy download en in terminal and restart the program running the code\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "!pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JQ7tDdefeLz",
    "outputId": "7b3d0f2f-320f-413c-9830-1c68c735ba88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\nbelo\\AppData\\Local\\Temp/ipykernel_16604/2160679219.py:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if p[1][0] is 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if p[1][0] is 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(p[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    return p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the Jaccard Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "\n",
    "def remove_punct_and_lower(s0, s1):\n",
    "    s0 = [w.lower() for w in s0 if not all(c in punct for c in w)]\n",
    "    s1 = [w.lower() for w in s1 if not all(c in punct for c in w)]\n",
    "    return s0, s1\n",
    "\n",
    "computed_synsets = {}\n",
    "\n",
    "def get_wordnet_similarity(s0, s1, method):\n",
    "    if method == \"path\" and s0 is not None and s1 is not None:\n",
    "        return s0.path_similarity(s1)\n",
    "    elif method == \"lch\" and s0 is not None and s1 is not None and s0.pos == s1.pos:\n",
    "        return s0.lch_similarity(s1)\n",
    "    elif method == \"wup\" and s0 is not None and s1 is not None:\n",
    "        return s0.wup_similarity(s1)\n",
    "    elif method == \"lin\" and s0 is not None and s1 is not None and s1.pos == s1.pos and s0.pos in {'n', 'v', 'r', 'a'}:\n",
    "        return s0.lin_similarity(s1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def max_similarity(s0, s1, method):\n",
    "    if s0 == s1:\n",
    "        return 1\n",
    "\n",
    "    if (s0, s1, method) in computed_synsets:\n",
    "        return computed_synsets[(s0, s1, method)]\n",
    "\n",
    "    synsets0 = wordnet.synsets(s0)\n",
    "    synsets1 = wordnet.synsets(s1)\n",
    "\n",
    "    similarities = []\n",
    "    for syn0 in synsets0:\n",
    "        for syn1 in synsets1:\n",
    "            similarity = get_wordnet_similarity(syn0, syn1, method)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    if len(similarities) > 0:\n",
    "        max_sim = max(similarities)\n",
    "        if method == 'lch':\n",
    "            # If Leacock similarity divide by 3 in order to normalize the similarity\n",
    "            computed_synsets[(s0, s1, method)] = max_sim/3\n",
    "            return max_sim/3\n",
    "        else:\n",
    "            computed_synsets[(s0, s1, method)] = max_sim\n",
    "            return max_sim\n",
    "    else:\n",
    "        computed_synsets[(s0, s1, method)] = 0\n",
    "        return 0\n",
    "\n",
    "def synset_similarity(lemmas0, lemmas1, method):\n",
    "    # TODO\n",
    "    return None\n",
    "\n",
    "def compute_jaccard(sentence_0, sentence_1, remove_punctuation=True, type='nltk'):\n",
    "    s0 = nltk.word_tokenize(sentence_0)\n",
    "    s1 = nltk.word_tokenize(sentence_1)\n",
    "\n",
    "    if remove_punctuation:\n",
    "        s0, s1 = remove_punct_and_lower(s0, s1)\n",
    "\n",
    "    if type == 'lemma':\n",
    "        return lemmatize_sentences(s0, s1)\n",
    "\n",
    "    elif type == \"NE\":\n",
    "        s0_ne = name_entity_tokenization(sentence_0)\n",
    "\n",
    "        s1_ne = name_entity_tokenization(sentence_1)\n",
    "\n",
    "        return 1 - jaccard_distance(set(s0_ne), set(s1_ne))\n",
    "\n",
    "    elif type == \"Leacock\" or type == \"Lin\":\n",
    "        pairs_s0 = nltk.pos_tag(s0)\n",
    "        pairs_s1 = nltk.pos_tag(s1)\n",
    "\n",
    "        syn = []\n",
    "        for pair in pairs_s0:\n",
    "            try:\n",
    "                syn.append(wn.synsets(pair[0], pair[1].lower()[0])[0])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if type == \"Leacock\":\n",
    "            lSimA = round(syn[0].lch_similarity(syn[0]), 3)\n",
    "            try:\n",
    "                lSim = round(word.lch_similarity(word2) / lSimA, 3)\n",
    "            except:\n",
    "                lSimA.append(0)\n",
    "        if type == \"Lin\":\n",
    "            liSimA = round(syn[0].lin_similarity(syn[0], brown_ic), 3)\n",
    "            try:\n",
    "                liSim = round(word.lin_similarity(word2, brown_ic) / liSimA, 3)\n",
    "            except:\n",
    "                liSimA.append(0)\n",
    "\n",
    "\n",
    "def name_entity_tokenization(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        tokens = [token for token in doc]\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(doc[ent.start:ent.end],\n",
    "                              attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "    s0_ne = [token.text for token in doc]\n",
    "    return s0_ne\n",
    "\n",
    "\n",
    "def lemmatize_sentences(s0, s1):\n",
    "    pairs_s0 = nltk.pos_tag(s0)\n",
    "    pairs_s1 = nltk.pos_tag(s1)\n",
    "    s0 = [lemmatize(pair) for pair in pairs_s0]\n",
    "    s1 = [lemmatize(pair) for pair in pairs_s1]\n",
    "    return s0, s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train\\STS.input.MSRpar.txt\n",
      "data\\train\\STS.input.MSRvid.txt\n",
      "data\\train\\STS.input.SMTeuroparl.txt\n",
      "data\\train\\STS.gs.MSRpar.txt\n",
      "data\\train\\STS.gs.MSRvid.txt\n",
      "data\\train\\STS.gs.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.input.MSRpar.txt\n",
      "data\\test-gold\\STS.input.MSRvid.txt\n",
      "data\\test-gold\\STS.input.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.input.surprise.OnWN.txt\n",
      "data\\test-gold\\STS.input.surprise.SMTnews.txt\n",
      "data\\test-gold\\STS.gs.ALL.txt\n",
      "data\\test-gold\\STS.gs.MSRpar.txt\n",
      "data\\test-gold\\STS.gs.MSRvid.txt\n",
      "data\\test-gold\\STS.gs.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.gs.surprise.OnWN.txt\n",
      "data\\test-gold\\STS.gs.surprise.SMTnews.txt\n"
     ]
    }
   ],
   "source": [
    "#path = os.path.join('data', 'train', 'STS.input.MSRpar.txt')\n",
    "\n",
    "#df_input = read_file(path)\n",
    "#df_input.head()\n",
    "\n",
    "\n",
    "#Train data\n",
    "inputTexts=[]\n",
    "dataPath=os.path.join('data', 'train')\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print (os.path.join(dataPath,filename))\n",
    "        inputTexts.append(read_file(os.path.join(dataPath,filename)))\n",
    "\n",
    "inputGs=[]\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.gs\" in filename:\n",
    "        print (os.path.join(dataPath,filename))\n",
    "        inputGs.append(read_file(os.path.join(dataPath,filename)))\n",
    "\n",
    "\n",
    "##Test data\n",
    "inputTexts=[]\n",
    "dataPath=os.path.join('data', 'test-gold')\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print (os.path.join(dataPath,filename))\n",
    "        inputTexts.append(read_file(os.path.join(dataPath,filename)))\n",
    "\n",
    "inputGs=[]\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.gs\" in filename:\n",
    "        print (os.path.join(dataPath,filename))\n",
    "        inputGs.append(read_file(os.path.join(dataPath,filename)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16604/1106316713.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import the model we are using\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Instantiate model with 1000 decision trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Train the model on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 6, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "regr = svm.SVR()\n",
    "regr.fit(X, y)\n",
    "SVR()\n",
    "regr.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "model.fit(x_train, y_train)  #fit the model\n",
    "pred=model.predict(x_test) #make prediction on test set"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
