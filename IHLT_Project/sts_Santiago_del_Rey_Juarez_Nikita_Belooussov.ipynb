{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey Ju√°rez\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgling in c:\\users\\nbelo\\anaconda3\\envs\\ihlt\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: svgwrite in c:\\users\\nbelo\\anaconda3\\envs\\ihlt\\lib\\site-packages (from svgling) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import spacy\n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") #if this does not work run python -m spacy download en in terminal and restart the program running the code\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "!pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JQ7tDdefeLz",
    "outputId": "7b3d0f2f-320f-413c-9830-1c68c735ba88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\santi\\AppData\\Local\\Temp/ipykernel_39864/1815987616.py:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if p[1][0] is 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if p[1][0] is 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(p[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    return p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the Jaccard Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "\n",
    "def remove_punct_and_lower(s0, s1):\n",
    "    s0 = [w.lower() for w in s0 if not all(c in punct for c in w)]\n",
    "    s1 = [w.lower() for w in s1 if not all(c in punct for c in w)]\n",
    "    return s0, s1\n",
    "\n",
    "computed_synsets = {}\n",
    "\n",
    "def get_wordnet_similarity(s0, s1, method):\n",
    "    if method == \"path\" and s0 is not None and s1 is not None:\n",
    "        return s0.path_similarity(s1)\n",
    "    elif method == \"lch\" and s0 is not None and s1 is not None and s0.pos == s1.pos:\n",
    "        return s0.lch_similarity(s1)\n",
    "    elif method == \"wup\" and s0 is not None and s1 is not None:\n",
    "        return s0.wup_similarity(s1)\n",
    "    elif method == \"lin\" and s0 is not None and s1 is not None and s1.pos == s1.pos and s0.pos in {'n', 'v', 'r', 'a'}:\n",
    "        return s0.lin_similarity(s1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def max_similarity(s0, s1, method):\n",
    "    if s0 == s1:\n",
    "        return 1\n",
    "\n",
    "    if (s0, s1, method) in computed_synsets:\n",
    "        return computed_synsets[(s0, s1, method)]\n",
    "\n",
    "    synsets0 = wordnet.synsets(s0)\n",
    "    synsets1 = wordnet.synsets(s1)\n",
    "\n",
    "    similarities = []\n",
    "    for syn0 in synsets0:\n",
    "        for syn1 in synsets1:\n",
    "            similarity = get_wordnet_similarity(syn0, syn1, method)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    if len(similarities) > 0:\n",
    "        max_sim = max(similarities)\n",
    "        if method == 'lch':\n",
    "            # If Leacock similarity divide by 3 in order to normalize the similarity\n",
    "            computed_synsets[(s0, s1, method)] = max_sim/3\n",
    "            return max_sim/3\n",
    "        else:\n",
    "            computed_synsets[(s0, s1, method)] = max_sim\n",
    "            return max_sim\n",
    "    else:\n",
    "        computed_synsets[(s0, s1, method)] = 0\n",
    "        return 0\n",
    "\n",
    "def synset_similarity(lemmas0, lemmas1, method):\n",
    "    # TODO\n",
    "    return None\n",
    "\n",
    "def compute_jaccard(sentence_0, sentence_1, remove_punctuation=True, type='nltk'):\n",
    "    s0 = nltk.word_tokenize(sentence_0)\n",
    "    s1 = nltk.word_tokenize(sentence_1)\n",
    "\n",
    "    if remove_punctuation:\n",
    "        s0, s1 = remove_punct_and_lower(s0, s1)\n",
    "\n",
    "    if type == 'lemma':\n",
    "        return lemmatize_sentences(s0, s1)\n",
    "\n",
    "    elif type == \"NE\":\n",
    "        s0_ne = name_entity_tokenization(sentence_0)\n",
    "\n",
    "        s1_ne = name_entity_tokenization(sentence_1)\n",
    "\n",
    "        return 1 - jaccard_distance(set(s0_ne), set(s1_ne))\n",
    "\n",
    "    elif type == \"Leacock\" or type == \"Lin\":\n",
    "        pairs_s0 = nltk.pos_tag(s0)\n",
    "        pairs_s1 = nltk.pos_tag(s1)\n",
    "\n",
    "        syn = []\n",
    "        for pair in pairs_s0:\n",
    "            try:\n",
    "                syn.append(wn.synsets(pair[0], pair[1].lower()[0])[0])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if type == \"Leacock\":\n",
    "            lSimA = round(syn[0].lch_similarity(syn[0]), 3)\n",
    "            try:\n",
    "                lSim = round(word.lch_similarity(word2) / lSimA, 3)\n",
    "            except:\n",
    "                lSimA.append(0)\n",
    "        if type == \"Lin\":\n",
    "            liSimA = round(syn[0].lin_similarity(syn[0], brown_ic), 3)\n",
    "            try:\n",
    "                liSim = round(word.lin_similarity(word2, brown_ic) / liSimA, 3)\n",
    "            except:\n",
    "                liSimA.append(0)\n",
    "\n",
    "\n",
    "def name_entity_tokenization(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        tokens = [token for token in doc]\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(doc[ent.start:ent.end],\n",
    "                              attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "    s0_ne = [token.text for token in doc]\n",
    "    return s0_ne\n",
    "\n",
    "\n",
    "def lemmatize_sentences(s0, s1):\n",
    "    pairs_s0 = nltk.pos_tag(s0)\n",
    "    pairs_s1 = nltk.pos_tag(s1)\n",
    "    s0 = [lemmatize(pair) for pair in pairs_s0]\n",
    "    s1 = [lemmatize(pair) for pair in pairs_s1]\n",
    "    return s0, s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But other sources close to the sale said Viven...</td>\n",
       "      <td>But other sources close to the sale said Viven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Micron has declared its first quarterly profit...</td>\n",
       "      <td>Micron's numbers also marked the first quarter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The fines are part of failed Republican effort...</td>\n",
       "      <td>Perry said he backs the Senate's efforts, incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The American Anglican Council, which represent...</td>\n",
       "      <td>The American Anglican Council, which represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n",
       "      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence0  \\\n",
       "0  But other sources close to the sale said Viven...   \n",
       "1  Micron has declared its first quarterly profit...   \n",
       "2  The fines are part of failed Republican effort...   \n",
       "3  The American Anglican Council, which represent...   \n",
       "4  The tech-loaded Nasdaq composite rose 20.96 po...   \n",
       "\n",
       "                                           sentence1  \n",
       "0  But other sources close to the sale said Viven...  \n",
       "1  Micron's numbers also marked the first quarter...  \n",
       "2  Perry said he backs the Senate's efforts, incl...  \n",
       "3  The American Anglican Council, which represent...  \n",
       "4  The technology-laced Nasdaq Composite Index <....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join('data', 'train', 'STS.input.MSRpar.txt')\n",
    "\n",
    "df_input = read_file(path)\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 6, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "regr = svm.SVR()\n",
    "regr.fit(X, y)\n",
    "SVR()\n",
    "regr.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "model.fit(x_train, y_train)  #fit the model\n",
    "pred=model.predict(x_test) #make prediction on test set"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
