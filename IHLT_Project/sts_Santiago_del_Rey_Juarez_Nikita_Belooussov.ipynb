{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey JuÃ¡rez\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import string\n",
    "from enum import Enum\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\")  #if this does not work run python -m spacy download en in terminal and restart the program running the code\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.lower() for w in nltk.word_tokenize(sentence) if\n",
    "            not all(c in punct for c in w) and w.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def name_entity_tokenization(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        tokens = [token for token in doc]\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(doc[ent.start:ent.end],\n",
    "                              attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "    s0_ne = [token.text for token in doc]\n",
    "    return s0_ne"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(pair):\n",
    "    if pair[1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if pair[1][0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(pair[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(pair[0].lower(), pos=pair[1][0].lower())\n",
    "    return pair[0]\n",
    "\n",
    "\n",
    "def lemmatize_sentence(words):\n",
    "    pairs = nltk.pos_tag(words)\n",
    "    lemmas = [lemmatize(pair) for pair in pairs]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Synset similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_similarity(s0, s1, method):\n",
    "    if method == \"path\" and s0 is not None and s1 is not None:\n",
    "        return s0.path_similarity(s1)\n",
    "    elif method == \"lch\" and s0 is not None and s1 is not None and s0.pos == s1.pos:\n",
    "        return s0.lch_similarity(s1)\n",
    "    elif method == \"wup\" and s0 is not None and s1 is not None:\n",
    "        return s0.wup_similarity(s1)\n",
    "    elif method == \"lin\" and s0 is not None and s1 is not None and s1.pos == s1.pos and s0.pos in {'n', 'v', 'r', 'a'}:\n",
    "        return s0.lin_similarity(s1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Dictionary used to store already computed synsets\n",
    "computed_synsets = {}\n",
    "\n",
    "\n",
    "def max_similarity(s0, s1, method):\n",
    "    if s0 == s1:\n",
    "        return 1\n",
    "\n",
    "    if (s0, s1, method) in computed_synsets:\n",
    "        return computed_synsets[(s0, s1, method)]\n",
    "\n",
    "    synsets0 = wordnet.synsets(s0)\n",
    "    synsets1 = wordnet.synsets(s1)\n",
    "\n",
    "    similarities = []\n",
    "    for syn0 in synsets0:\n",
    "        for syn1 in synsets1:\n",
    "            similarity = get_wordnet_similarity(syn0, syn1, method)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    if len(similarities) > 0:\n",
    "        max_sim = max(similarities)\n",
    "        if method == 'lch':\n",
    "            # If Leacock similarity divide by 3 in order to normalize the similarity\n",
    "            computed_synsets[(s0, s1, method)] = max_sim / 3\n",
    "            return max_sim / 3\n",
    "        else:\n",
    "            computed_synsets[(s0, s1, method)] = max_sim\n",
    "            return max_sim\n",
    "    else:\n",
    "        computed_synsets[(s0, s1, method)] = 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mean_simimilarity(lemmas0, lemmas1, method):\n",
    "    sum = 0\n",
    "    for l0 in lemmas0:\n",
    "        sum += max([max_similarity(l0, l1, method) for l1 in lemmas1])\n",
    "    return sum / len(lemmas0)\n",
    "\n",
    "\n",
    "def synset_similarity(lemmas0, lemmas1, method):\n",
    "    mean_sim0 = mean_simimilarity(lemmas0, lemmas1, method)\n",
    "    mean_sim1 = mean_simimilarity(lemmas1, lemmas0, method)\n",
    "\n",
    "    if mean_sim0 > 0 or mean_sim1 > 0:\n",
    "        return (2 * mean_sim0 * mean_sim1) / (mean_sim0 + mean_sim0)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lesk similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def lesk_similarity(words0, words1):\n",
    "    w0_pos = nltk.pos_tag(words0)\n",
    "    w1_pos = nltk.pos_tag(words1)\n",
    "\n",
    "    s0_lesk = []\n",
    "    for i in range(len(w0_pos)):\n",
    "        if w0_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w0_pos[i][1][0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=w0_pos[i][1][0].lower()))\n",
    "\n",
    "    s1_lesk = []\n",
    "    for i in range(len(w1_pos)):\n",
    "        if w1_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w1_pos[i][1][0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=w1_pos[i][1][0].lower()))\n",
    "\n",
    "    return 1 - jaccard_distance(set(s0_lesk), set(s1_lesk))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class Methods(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration with the available similarity methods\n",
    "    \"\"\"\n",
    "    JACCARD = 'jaccard'\n",
    "    NE = 'NE'\n",
    "    PATH = 'path'\n",
    "    LEACOCK = 'lch'\n",
    "    WU = 'wup'\n",
    "    LIN = 'lin'\n",
    "    LESK = 'lesk'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def compute_similarity(sentence_0, sentence_1, method='jaccard'):\n",
    "    words0 = tokenize(sentence_0)\n",
    "    words1 = tokenize(sentence_1)\n",
    "    s0_lemmas = lemmatize_sentence(words0)\n",
    "    s1_lemmas = lemmatize_sentence(words1)\n",
    "\n",
    "    if method == 'jaccard':\n",
    "        return 1 - jaccard_distance(set(s0_lemmas), set(s1_lemmas))\n",
    "\n",
    "    elif method == 'NE':\n",
    "        s0_ne = name_entity_tokenization(sentence_0)\n",
    "        s1_ne = name_entity_tokenization(sentence_1)\n",
    "        return 1 - jaccard_distance(set(s0_ne), set(s1_ne))\n",
    "\n",
    "    elif method in ['path', 'lch', 'wup', 'lin']:\n",
    "        return synset_similarity(s0_lemmas, s1_lemmas, method)\n",
    "\n",
    "    elif method == 'lesk':\n",
    "        return lesk_similarity(words0, words1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train\\STS.input.MSRpar.txt\n",
      "data\\train\\STS.input.MSRvid.txt\n",
      "data\\train\\STS.input.SMTeuroparl.txt\n",
      "data\\train\\STS.gs.MSRpar.txt\n",
      "data\\train\\STS.gs.MSRvid.txt\n",
      "data\\train\\STS.gs.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.input.MSRpar.txt\n",
      "data\\test-gold\\STS.input.MSRvid.txt\n",
      "data\\test-gold\\STS.input.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.input.surprise.OnWN.txt\n",
      "data\\test-gold\\STS.input.surprise.SMTnews.txt\n",
      "data\\test-gold\\STS.gs.ALL.txt\n",
      "data\\test-gold\\STS.gs.MSRpar.txt\n",
      "data\\test-gold\\STS.gs.MSRvid.txt\n",
      "data\\test-gold\\STS.gs.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.gs.surprise.OnWN.txt\n",
      "data\\test-gold\\STS.gs.surprise.SMTnews.txt\n"
     ]
    }
   ],
   "source": [
    "#path = os.path.join('data', 'train', 'STS.input.MSRpar.txt')\n",
    "\n",
    "#df_input = read_file(path)\n",
    "#df_input.head()\n",
    "\n",
    "\n",
    "#Train data\n",
    "inputTexts = []\n",
    "dataPath = os.path.join('data', 'train')\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        inputTexts.append(read_file(os.path.join(dataPath, filename)))\n",
    "\n",
    "inputGs = []\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.gs\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        inputGs.append(read_file(os.path.join(dataPath, filename)))\n",
    "\n",
    "##Test data\n",
    "inputTexts = []\n",
    "dataPath = os.path.join('data', 'test-gold')\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        inputTexts.append(read_file(os.path.join(dataPath, filename)))\n",
    "\n",
    "inputGs = []\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.gs\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        inputGs.append(read_file(os.path.join(dataPath, filename)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem likely will mean corrective changes before the shuttle fleet starts flying again.\n",
      "He said the problem needs to be corrected before the space shuttle fleet is cleared to fly again.\r\n",
      "Similarity using jaccard: 0.2857142857142857\n",
      "Similarity using NE: 0.28\n",
      "Similarity using path: 0.6518518518518519\n",
      "Similarity using lch: 0.4444444444444444\n",
      "Similarity using wup: 0.8361314611314613\n",
      "Similarity using lin: 0.4444444444444444\n",
      "Similarity using lesk: 0.0625\n"
     ]
    }
   ],
   "source": [
    "# TODO: Remove testing cell\n",
    "s0 = inputTexts[0].iloc[0,0]\n",
    "s1 = inputTexts[0].iloc[0,1]\n",
    "print(s0)\n",
    "print(s1)\n",
    "for method in Methods:\n",
    "    print(f'Similarity using {method.value}: {compute_similarity(s0, s1, method.value)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_15840/3007752581.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Import the model we are using\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensemble\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomForestRegressor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m# Instantiate model with 1000 decision trees\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mrf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mRandomForestRegressor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_estimators\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m6\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m42\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators=6, random_state=42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "regr = svm.SVR()\n",
    "regr.fit(X, y)\n",
    "SVR()\n",
    "regr.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsRegressor(n_neighbors=K)\n",
    "\n",
    "model.fit(x_train, y_train)  #fit the model\n",
    "pred = model.predict(x_test)  #make prediction on test set"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}