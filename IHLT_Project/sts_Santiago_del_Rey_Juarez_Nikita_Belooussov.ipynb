{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey Ju√°rez\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import spacy\n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "brown_ic=wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "!pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JQ7tDdefeLz",
    "outputId": "7b3d0f2f-320f-413c-9830-1c68c735ba88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N','V','J','R'}: #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if p[1][0] is 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(p[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    return p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the Jaccard Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct_and_lower(s0, s1):\n",
    "    s0 = [w.lower() for w in s0 if not all(c in punct for c in w)]\n",
    "    s1 = [w.lower() for w in s1 if not all(c in punct for c in w)]\n",
    "    return s0, s1\n",
    "\n",
    "def compute_jaccard(sentence_0, sentence_1, type = 'nltk'):\n",
    "    s0 = nltk.word_tokenize(sentence_0)\n",
    "    s1 = nltk.word_tokenize(sentence_1)\n",
    "\n",
    "    if type == 'lemma':\n",
    "        pairs_s0 = nltk.pos_tag(s0)\n",
    "        pairs_s1 = nltk.pos_tag(s1)\n",
    "        s0 = [lemmatize(pair) for pair in pairs_s0]\n",
    "        s1 = [lemmatize(pair) for pair in pairs_s1]\n",
    "    \n",
    "    if type == 'punct':\n",
    "        s0, s1 = remove_punct_and_lower(s0, s1)\n",
    "    return 1 - jaccard_distance(set(s0), set(s1))\n",
    "\n",
    "\n",
    "    if type == \"NE\":\n",
    "        doc = nlp(sentence_0.lower())\n",
    "        \n",
    "        with doc.retokenize() as retokenizer:\n",
    "            tokens = [token for token in doc]\n",
    "            for ent in doc.ents:\n",
    "                retokenizer.merge(doc[ent.start:ent.end], \n",
    "                          attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "        s0_ne = [token.text for token in doc]\n",
    "    \n",
    "        doc = nlp(sentence_1.lower())\n",
    "    \n",
    "        with doc.retokenize() as retokenizer:\n",
    "            tokens = [token for token in doc]\n",
    "            for ent in doc.ents:\n",
    "                retokenizer.merge(doc[ent.start:ent.end], \n",
    "                          attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "\n",
    "        s1_ne = [token.text for token in doc]\n",
    "\n",
    "        #print('Sentence 0:')\n",
    "        #print(s0_ne)\n",
    "        #print('Sentence 1:')\n",
    "        #print(s1_ne)\n",
    "\n",
    "        return 1 - jaccard_distance(set(s0_ne), set(s1_ne))\n",
    "    \n",
    "    if type==\"Leacock\" or type==\"Lin\":\n",
    "        syn=[]\n",
    "        for word in words:\n",
    "            try:\n",
    "                syn.append(wn.synsets(word[0],word[1].lower()[0])[0])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if type==\"Leacock\":\n",
    "            lSimA=round(syn[0].lch_similarity(syn[0]),3)\n",
    "            try:\n",
    "                lSim=round(word.lch_similarity(word2)/lSimA,3)\n",
    "            except:\n",
    "                lSimA.append(0)\n",
    "        if type==\"Lin\":\n",
    "            liSimA=round(syn[0].lin_similarity(syn[0],brown_ic),3)\n",
    "            try:\n",
    "                liSim=round(word.lin_similarity(word2,brown_ic)/liSimA,3)\n",
    "            except:\n",
    "                liSimA.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But other sources close to the sale said Viven...</td>\n",
       "      <td>But other sources close to the sale said Viven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Micron has declared its first quarterly profit...</td>\n",
       "      <td>Micron's numbers also marked the first quarter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The fines are part of failed Republican effort...</td>\n",
       "      <td>Perry said he backs the Senate's efforts, incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The American Anglican Council, which represent...</td>\n",
       "      <td>The American Anglican Council, which represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n",
       "      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence0  \\\n",
       "0  But other sources close to the sale said Viven...   \n",
       "1  Micron has declared its first quarterly profit...   \n",
       "2  The fines are part of failed Republican effort...   \n",
       "3  The American Anglican Council, which represent...   \n",
       "4  The tech-loaded Nasdaq composite rose 20.96 po...   \n",
       "\n",
       "                                           sentence1  \n",
       "0  But other sources close to the sale said Viven...  \n",
       "1  Micron's numbers also marked the first quarter...  \n",
       "2  Perry said he backs the Senate's efforts, incl...  \n",
       "3  The American Anglican Council, which represent...  \n",
       "4  The technology-laced Nasdaq Composite Index <....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join('data', 'train', 'STS.input.MSRpar.txt')\n",
    "\n",
    "\n",
    "\n",
    "df_input = read_file(path)\n",
    "df_input.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
