{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey JuÃ¡rez\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\santi\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (0.0.58)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\santi\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\santi\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: anyascii in c:\\users\\santi\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\santi\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\santi\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (from num2words) (0.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# requires visual studios builder from https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "!pip install contractions\n",
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "import num2words\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus.reader import WordNetError\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "contractions.add('U.S.', 'United States')\n",
    "contractions.add('U.S.A', 'United States of America')\n",
    "contractions.add('E.U.', 'European Union')\n",
    "\n",
    "#if this does not work run python -m spacy download en in terminal and restart the program running the code\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def expand_contractions(s0, s1):\n",
    "    s0 = contractions.fix(s0)\n",
    "    s1 = contractions.fix(s1)\n",
    "    return s0, s1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Change numbers to words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def changeNums(s0):\n",
    "    s0=s0.split()\n",
    "    new_s0=[]\n",
    "    for i in s0:\n",
    "        if i.isdigit():\n",
    "            new_s0.append(num2words.num2words(i))\n",
    "        else:\n",
    "            new_s0.append(i)\n",
    "    s0 = ' '.join(new_s0)\n",
    "    return s0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.lower() for w in nltk.word_tokenize(sentence) if\n",
    "            not all(c in punct for c in w) and w.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def name_entity_tokenization(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        tokens = [token for token in doc]\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(doc[ent.start:ent.end],\n",
    "                              attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "    s0_ne = [token.text for token in doc]\n",
    "    return s0_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(pair):\n",
    "    if pair[1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if pair[1][\n",
    "            0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(pair[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(pair[0].lower(), pos=pair[1][0].lower())\n",
    "    return pair[0]\n",
    "\n",
    "\n",
    "def lemmatize_sentence(words):\n",
    "    pairs = nltk.pos_tag(words)\n",
    "    lemmas = [lemmatize(pair) for pair in pairs]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Synset similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_similarity(s0, s1, method, ic):\n",
    "    if s0 is not None and s1 is not None:\n",
    "        if method == 'path':\n",
    "            return s0.path_similarity(s1)\n",
    "        elif method == 'wup':\n",
    "            return s0.wup_similarity(s1)\n",
    "        elif s0.pos() == s1.pos():\n",
    "            if method == \"lch\":\n",
    "                return s0.lch_similarity(s1)\n",
    "            elif ic is None:\n",
    "                raise ValueError(\"ic parameter is missing\")\n",
    "            elif method == \"lin\":\n",
    "                try:\n",
    "                    return s0.lin_similarity(s1, ic)\n",
    "                except WordNetError:\n",
    "                    return None\n",
    "            elif method == 'res':\n",
    "                try:\n",
    "                    return s0.res_similarity(s1, ic)\n",
    "                except WordNetError:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Dictionary used to store already computed synsets\n",
    "try:\n",
    "    with open('synset_dic.pkl', 'rb') as file:\n",
    "        computed_synsets = pickle.load(file)\n",
    "except IOError:\n",
    "    computed_synsets = {}\n",
    "\n",
    "\n",
    "def max_similarity(s0, s1, method, ic):\n",
    "    if s0 == s1:\n",
    "        return 1\n",
    "\n",
    "    if (s0, s1, method) in computed_synsets:\n",
    "        return computed_synsets[(s0, s1, method)]\n",
    "\n",
    "    synsets0 = wordnet.synsets(s0)\n",
    "    synsets1 = wordnet.synsets(s1)\n",
    "\n",
    "    similarities = []\n",
    "    for syn0 in synsets0:\n",
    "        for syn1 in synsets1:\n",
    "            similarity = get_wordnet_similarity(syn0, syn1, method, ic)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    if len(similarities) > 0:\n",
    "        max_sim = max(similarities)\n",
    "        computed_synsets[(s0, s1, method)] = max_sim\n",
    "        return max_sim\n",
    "    else:\n",
    "        computed_synsets[(s0, s1, method)] = 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mean_simimilarity(lemmas0, lemmas1, method, ic):\n",
    "    similarity_sum = 0\n",
    "    for l0 in lemmas0:\n",
    "        similarity_sum += max([max_similarity(l0, l1, method, ic) for l1 in lemmas1])\n",
    "    return similarity_sum / len(lemmas0)\n",
    "\n",
    "\n",
    "def synset_similarity(lemmas0, lemmas1, method, ic=None):\n",
    "    mean_sim0 = mean_simimilarity(lemmas0, lemmas1, method, ic)\n",
    "    mean_sim1 = mean_simimilarity(lemmas1, lemmas0, method, ic)\n",
    "\n",
    "    if mean_sim0 > 0 or mean_sim1 > 0:\n",
    "        return mean_sim0 + mean_sim1 / 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lesk similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lesk_similarity(words0, words1):\n",
    "    w0_pos = nltk.pos_tag(words0)\n",
    "    w1_pos = nltk.pos_tag(words1)\n",
    "\n",
    "    s0_lesk = []\n",
    "    for i in range(len(w0_pos)):\n",
    "        if w0_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w0_pos[i][1][\n",
    "                0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=w0_pos[i][1][0].lower()))\n",
    "\n",
    "    s1_lesk = []\n",
    "    for i in range(len(w1_pos)):\n",
    "        if w1_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w1_pos[i][1][\n",
    "                0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=w1_pos[i][1][0].lower()))\n",
    "\n",
    "    return 1 - jaccard_distance(set(s0_lesk), set(s1_lesk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(words0, words1):\n",
    "    return 1 - jaccard_distance(set(words0), set(words1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ninputTexts = []\\ndataPath = os.path.join(\\'data\\', \\'train\\')\\nx = []\\nfor filename in os.listdir(dataPath):\\n    if \"STS.input\" in filename:\\n        print(os.path.join(dataPath, filename))\\n        file = os.path.join(dataPath, filename)\\n        text = pd.read_csv(file, sep=\\'\\t\\', lineterminator=\\'\\n\\', names=[\\'sentence0\\', \\'sentence1\\'], header=None,\\n                           quoting=csv.QUOTE_NONE)\\n        for i in range(len(text[\\'sentence0\\'])):\\n            metric = calc_syn(text[\\'sentence0\\'][i], text[\\'sentence1\\'][i])\\n            x.append(metric)\\n        inputTexts.append(text)\\n#print (\"avg:\")\\nnumSen = len(x)\\nx = np.array(x)\\n#print(sum(x[:, 0]) / numSen)\\n#print(sum(x[:, 1]) / numSen)\\n#print(sum(x[:, 2]) / numSen)\\n#print(sum(x[:, 3]) / numSen)\\n#print(sum(x[:, 4]) / numSen)\\n#print(sum(x[:, 5]) / numSen)'"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_syn(s0, s1):\n",
    "    if len(s1) < len(s0):\n",
    "        s0, s1 = s1, s0\n",
    "    #s0, s1 = expand_contractions(s0, s1)\n",
    "    #s0 = tokenize(s0)\n",
    "    #s1 = tokenize(s1)\n",
    "    #s0 = lemmatize_sentence(s0)\n",
    "    #s1 = lemmatize_sentence(s1)\n",
    "\n",
    "    synonyms1 = []\n",
    "    synonyms2 = []\n",
    "    for i in s0:\n",
    "        synonyms1 = [*synonyms1, *wordnet.synsets(i)]\n",
    "    for i in s1:\n",
    "        synonyms2 = [*synonyms2, *wordnet.synsets(i)]\n",
    "\n",
    "    count = 0\n",
    "    for i in synonyms1:\n",
    "        if i in synonyms2:\n",
    "            count = count + 1\n",
    "    if (len(synonyms1) != 0) and (len(synonyms2) != 0):\n",
    "        return count / len(synonyms1)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TF IDF and cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tf_similarity(s0, s1):\n",
    "    # Generate the tf-idf vectors for the corpus\n",
    "    words0 = ' '.join([str(elem) for elem in s0])\n",
    "    words1 = ' '.join([str(elem) for elem in s1])\n",
    "\n",
    "    tfvec = TfidfVectorizer()\n",
    "    tfidf_matrix = tfvec.fit_transform([words0, words1])\n",
    "\n",
    "    return cosine_similarity(tfidf_matrix, tfidf_matrix)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### N-Gram similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ninputTexts = []\\ndataPath = os.path.join(\\'data\\', \\'train\\')\\nx = []\\nfor filename in os.listdir(dataPath):\\n    if \"STS.input\" in filename:\\n        print(os.path.join(dataPath, filename))\\n        file = os.path.join(dataPath, filename)\\n        text = pd.read_csv(file, sep=\\'\\t\\', lineterminator=\\'\\n\\', names=[\\'sentence0\\', \\'sentence1\\'], header=None,\\n                           quoting=csv.QUOTE_NONE)\\n        for i in range(len(text[\\'sentence0\\'])):\\n            metric = compute_n_grams(text[\\'sentence0\\'][i], text[\\'sentence1\\'][i])\\n            x.append(metric)\\n        inputTexts.append(text)\\n#print (\"avg:\")\\nnumSen = len(x)\\nx = np.array(x)\\nprint(sum(x[:, 0]) / numSen)\\nprint(sum(x[:, 1]) / numSen)\\nprint(sum(x[:, 2]) / numSen)\\nprint(sum(x[:, 3]) / numSen)\\nprint(sum(x[:, 4]) / numSen)\\nprint(sum(x[:, 5]) / numSen)'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_n_grams(s0, s1):\n",
    "    #words1=[word for word in s0.split(\" \") if word not in set(stopwords.words('english'))]\n",
    "    low_size = 5\n",
    "    if len(s1) < len(s0):\n",
    "        s0, s1 = s1, s0\n",
    "    if len(s0) < 5:\n",
    "        low_size = len(s0)\n",
    "    #s0, s1 = expand_contractions(s0, s1)\n",
    "    #s0 = tokenize(s0)\n",
    "    #s1 = tokenize(s1)\n",
    "    #s0 = lemmatize_sentence(s0)\n",
    "    #s1 = lemmatize_sentence(s1)\n",
    "    #print (s0)\n",
    "    #print (s1)\n",
    "    #print(\"Sentence after removing stopwords:\",s0)\n",
    "    metrics = [0, 0, 0, 0, 0, 0]\n",
    "    for i in range(2, low_size):\n",
    "        n_grams1 = zip(*[s0[k:] for k in range(0, i)])\n",
    "        n_grams2 = zip(*[s1[k:] for k in range(0, i)])\n",
    "        count = 0\n",
    "\n",
    "        n_grams1 = [' '.join(ngram) for ngram in n_grams1]\n",
    "        n_grams2 = [' '.join(ngram) for ngram in n_grams2]\n",
    "        #print (set(nGrams2))\n",
    "        #print (set(nGrams1))\n",
    "        for j in set(n_grams1):\n",
    "            if j in set(n_grams2):\n",
    "                count += 1\n",
    "        if (len(n_grams1) != 0) and (len(n_grams2) != 0):\n",
    "            #print(1-jaccard_distance(set(nGrams1), set(nGrams2)))\n",
    "            #print (count)\n",
    "            metrics[(i - 2) * 2] = count / len(set(n_grams1))\n",
    "            metrics[((i - 2) * 2) + 1] = 1 - jaccard_distance(set(n_grams1), set(n_grams2))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_SYMBOLS = 50\n",
    "\n",
    "\n",
    "def compute_similarity(x):\n",
    "    features = []\n",
    "    n_samples = x.shape[0]\n",
    "    perc = round(0.02 * n_samples)\n",
    "    counter = 0\n",
    "    progress = 0\n",
    "    for sentence_0, sentence_1 in x:\n",
    "        sentence_0=changeNums(sentence_0)\n",
    "        sentence_1=changeNums(sentence_1)\n",
    "        sentence_0, sentence_1 = expand_contractions(sentence_0, sentence_1)\n",
    "        words0 = tokenize(sentence_0)\n",
    "        words1 = tokenize(sentence_1)\n",
    "        s0_lemmas = lemmatize_sentence(words0)\n",
    "        s1_lemmas = lemmatize_sentence(words1)\n",
    "        s0_ne = name_entity_tokenization(sentence_0)\n",
    "        s1_ne = name_entity_tokenization(sentence_1)\n",
    "        n_grams_results = compute_n_grams(s0_lemmas, s1_lemmas)\n",
    "        features.append([\n",
    "            jaccard_similarity(words0, words1),\n",
    "            jaccard_similarity(s0_lemmas, s1_lemmas),\n",
    "            jaccard_similarity(s0_ne, s1_ne),\n",
    "            tf_similarity(words0, words1),\n",
    "            tf_similarity(s0_lemmas, s1_lemmas),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'path'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lch'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'wup'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lin', brown_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lin', semcor_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'res', brown_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'res', semcor_ic),\n",
    "            lesk_similarity(words0, words1),\n",
    "            n_grams_results[0],\n",
    "            n_grams_results[1],\n",
    "            n_grams_results[2],\n",
    "            n_grams_results[3],\n",
    "            n_grams_results[4],\n",
    "            n_grams_results[5],\n",
    "            calc_syn(s0_lemmas, s1_lemmas)\n",
    "        ])\n",
    "\n",
    "        progress = print_progress(counter, perc, progress)\n",
    "        counter += 1\n",
    "\n",
    "    print()\n",
    "    return np.array(features, dtype=np.float64)\n",
    "\n",
    "\n",
    "def print_progress(counter, perc, progress):\n",
    "    if (counter % perc) == 0:\n",
    "        print('<' + '#' * progress + '.' * (N_SYMBOLS - progress) + '>', end='\\r')\n",
    "        return progress + 1\n",
    "    return progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Train data\n",
    "dataPath = os.path.join('data', 'train')\n",
    "train_data = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.input\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if train_data is None:\n",
    "            train_data = data\n",
    "        else:\n",
    "            train_data = np.concatenate((train_data, data))\n",
    "\n",
    "y_train = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.gs\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if y_train is None:\n",
    "            y_train = data\n",
    "        else:\n",
    "            y_train = np.concatenate((y_train, data))\n",
    "\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "##Test data\n",
    "dataPath = os.path.join('data', 'test-gold')\n",
    "test_data = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.input\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if test_data is None:\n",
    "            test_data = data\n",
    "        else:\n",
    "            test_data = np.concatenate((test_data, data))\n",
    "\n",
    "y_test = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.gs\" in filename and \"ALL\" not in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if y_test is None:\n",
    "            y_test = data\n",
    "        else:\n",
    "            y_test = np.concatenate((y_test, data))\n",
    "\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation of training data similarities\n",
      "<#################################################.>\r\n",
      "Finished computation of training data similarities\n",
      "\n",
      "Starting computation of testing data similarities\n",
      "<..................................................>\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1015: RuntimeWarning: overflow encountered in square\n",
      "  temp **= 2\n",
      "C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1021: RuntimeWarning: overflow encountered in square\n",
      "  new_unnormalized_variance -= correction ** 2 / new_sample_count\n",
      "C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1021: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_unnormalized_variance -= correction ** 2 / new_sample_count\n",
      "C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:80: RuntimeWarning: overflow encountered in square\n",
      "  upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<##################################################>\r\n",
      "Finished computation of testing data similarities\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1015: RuntimeWarning: overflow encountered in square\n",
      "  temp **= 2\n",
      "C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1021: RuntimeWarning: overflow encountered in square\n",
      "  new_unnormalized_variance -= correction ** 2 / new_sample_count\n",
      "C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1021: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_unnormalized_variance -= correction ** 2 / new_sample_count\n",
      "C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:80: RuntimeWarning: overflow encountered in square\n",
      "  upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n"
     ]
    }
   ],
   "source": [
    "INF = np.finfo(float).max / 1e300\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print('Starting computation of training data similarities')\n",
    "train_features = compute_similarity(train_data)\n",
    "train_features[train_features == np.inf] = INF\n",
    "x_train = np.round(scaler.fit_transform(train_features), 3)\n",
    "print('Finished computation of training data similarities\\n')\n",
    "\n",
    "print('Starting computation of testing data similarities')\n",
    "test_features = compute_similarity(test_data)\n",
    "test_features[test_features == np.inf] = INF\n",
    "x_test = np.round(scaler.fit_transform(test_features), 3)\n",
    "print('Finished computation of testing data similarities\\n')\n",
    "\n",
    "# We save the already computed synset similarities to speed up future runs\n",
    "with open('synset_dic.pkl', 'wb') as file:\n",
    "    pickle.dump(computed_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selected_features = [0,  # jaccard using words\n",
    "                     1,  # jaccard using lemmas\n",
    "                     2,  # jaccard using NEs\n",
    "                     3,  # tf similarity using words\n",
    "                     4,  # tf similairty using lemmas\n",
    "                     5,  # path similarity\n",
    "                     6,  # lch similarity\n",
    "                     7,  # wup similarity\n",
    "                     #  8, # lin brown similarity\n",
    "                     9,  # lin semcor similarity\n",
    "                     #  10, # res brown similarity\n",
    "                     #  11, # res semcor similarity\n",
    "                     #  12 # lesk similarity\n",
    "                     13,  #n-Grams 0\n",
    "                     14,  #n-Grams 1\n",
    "                     15,  #n-grams 2\n",
    "                     16,  #ngrams 3\n",
    "                     17,  #ngrams 4\n",
    "                     18,  #ngrams 5\n",
    "                     #19 #synnonyms\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters: {'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 139}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=np.arange(135, 145), max_features=['auto', 'sqrt'],\n",
    "                  min_samples_leaf=np.arange(1, 10))\n",
    "\n",
    "rf_regr = GridSearchCV(RandomForestRegressor(), param_grid, n_jobs=-1)\n",
    "rf_regr.fit(x_train[:, selected_features], y_train)\n",
    "\n",
    "print(f'Optimal parameters: {rf_regr.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Random Forest Regressor: 0.6037846811081233\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "rf_pred = rf_regr.predict(x_test[:, selected_features])\n",
    "# Calculate the absolute errors\n",
    "rf_correlation = pearsonr(rf_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Random Forest Regressor: {rf_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "param_grid = dict(C=np.linspace(500, 550, 100), gamma=np.linspace(1e-5, 1e-2, 20))\n",
    "\n",
    "svr_regr = GridSearchCV(SVR(), param_grid, n_jobs=-1)\n",
    "svr_regr.fit(x_train[:, selected_features], y_train)\n",
    "\n",
    "print('Optimal parameters:', svr_regr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svr_pred = svr_regr.predict(x_test[:, selected_features])\n",
    "svr_correlation = pearsonr(svr_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Support Vector Regressor: {svr_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "param_grid = dict(n_neighbors=np.arange(1, 15), weights=['uniform', 'distance'],\n",
    "                  metric=['minkowski', 'euclidean', 'manhattan'])\n",
    "\n",
    "knn_regr = GridSearchCV(KNeighborsRegressor(), param_grid, n_jobs=-1)\n",
    "knn_regr.fit(x_train[:, selected_features], y_train)\n",
    "\n",
    "print(f'Optimal parameters: {knn_regr.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_pred = knn_regr.predict(x_test[:, selected_features])\n",
    "\n",
    "knn_correlation = pearsonr(knn_pred, y_test)[0]\n",
    "print(f'Pearson correlation using KNN Regressor: {knn_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "param_grid = dict(hidden_layer_sizes=[(50, 50, 50), (50, 100, 50), (100, 1)], activation=['relu', 'tanh', 'logistic'],\n",
    "                  alpha=np.linspace(1e-4, 0.1, 20), learning_rate=['constant', 'adaptive'], solver=['sgd', 'adam'])\n",
    "\n",
    "mlp_regr = GridSearchCV(MLPRegressor(random_state=1), param_grid, n_jobs=-1)\n",
    "mlp_regr.fit(x_train[:, selected_features], y_train)\n",
    "\n",
    "print(f'Optimal parameters: {mlp_regr.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_pred = mlp_regr.predict(x_test[:, selected_features])\n",
    "\n",
    "nn_correlation = pearsonr(nn_pred, y_test)[0]\n",
    "print(f'Pearson correlation using MLP: {nn_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lasso_alphas = np.linspace(0, 1, 20)\n",
    "\n",
    "lasso_regr = LassoCV(alphas=lasso_alphas, n_jobs=-1)\n",
    "lasso_regr.fit(x_train[:, selected_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso_pred = lasso_regr.predict(x_test[:, selected_features])\n",
    "\n",
    "lasso_correlation = pearsonr(lasso_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Lasso: {lasso_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "avg_pred = lasso_pred + nn_pred + svr_pred\n",
    "\n",
    "avg_correlation = pearsonr(avg_pred, y_test)[0]\n",
    "print(f'Pearson correlation using the average between Lasso, MLP and SVR: {avg_correlation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}