{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey JuÃ¡rez\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgling in c:\\users\\nbelo\\anaconda3\\envs\\ihlt2\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: svgwrite in c:\\users\\nbelo\\anaconda3\\envs\\ihlt2\\lib\\site-packages (from svgling) (1.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install svgling \n",
    "!pip install contractions # requires visual studios builder from https://visualstudio.microsoft.com/visual-cpp-build-tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import string\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "contractions.add('U.S.', 'United States')\n",
    "contractions.add('U.S.A', 'United States of America')\n",
    "contractions.add('E.U.', 'European Union')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #if this does not work run python -m spacy download en in terminal and restart the program running the code\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(s0,s1):\n",
    "    s0=contractions.fix(s0)\n",
    "    s1=contractions.fix(s1)\n",
    "    return s0,s1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.lower() for w in nltk.word_tokenize(sentence) if\n",
    "            not all(c in punct for c in w) and w.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def name_entity_tokenization(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        tokens = [token for token in doc]\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(doc[ent.start:ent.end],\n",
    "                              attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "    s0_ne = [token.text for token in doc]\n",
    "    return s0_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(pair):\n",
    "    if pair[1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if pair[1][0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(pair[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(pair[0].lower(), pos=pair[1][0].lower())\n",
    "    return pair[0]\n",
    "\n",
    "\n",
    "def lemmatize_sentence(words):\n",
    "    pairs = nltk.pos_tag(words)\n",
    "    lemmas = [lemmatize(pair) for pair in pairs]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Synset similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import WordNetError\n",
    "\n",
    "\n",
    "def get_wordnet_similarity(s0, s1, method):\n",
    "    if method == \"path\" and s0 is not None and s1 is not None:\n",
    "        return s0.path_similarity(s1)\n",
    "    elif method == \"lch\" and s0 is not None and s1 is not None and s0.pos() == s1.pos():\n",
    "        return s0.lch_similarity(s1)\n",
    "    elif method == \"wup\" and s0 is not None and s1 is not None:\n",
    "        return s0.wup_similarity(s1)\n",
    "    elif method == \"lin\" and s0 is not None and s1 is not None and s0.pos() == s1.pos():\n",
    "        try:\n",
    "            return s0.lin_similarity(s1, brown_ic)\n",
    "        except WordNetError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Dictionary used to store already computed synsets\n",
    "computed_synsets = {}\n",
    "\n",
    "\n",
    "def max_similarity(s0, s1, method):\n",
    "    if s0 == s1:\n",
    "        return 1\n",
    "\n",
    "    if (s0, s1, method) in computed_synsets:\n",
    "        return computed_synsets[(s0, s1, method)]\n",
    "\n",
    "    synsets0 = wordnet.synsets(s0)\n",
    "    synsets1 = wordnet.synsets(s1)\n",
    "\n",
    "    similarities = []\n",
    "    for syn0 in synsets0:\n",
    "        for syn1 in synsets1:\n",
    "            similarity = get_wordnet_similarity(syn0, syn1, method)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    if len(similarities) > 0:\n",
    "        max_sim = max(similarities)\n",
    "        if method == 'lch':\n",
    "            # If Leacock similarity divide by 3 in order to normalize the similarity\n",
    "            computed_synsets[(s0, s1, method)] = max_sim / 3\n",
    "            return max_sim / 3\n",
    "        else:\n",
    "            computed_synsets[(s0, s1, method)] = max_sim\n",
    "            return max_sim\n",
    "    else:\n",
    "        computed_synsets[(s0, s1, method)] = 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mean_simimilarity(lemmas0, lemmas1, method):\n",
    "    sum = 0\n",
    "    for l0 in lemmas0:\n",
    "        sum += max([max_similarity(l0, l1, method) for l1 in lemmas1])\n",
    "    return sum / len(lemmas0)\n",
    "\n",
    "\n",
    "def synset_similarity(lemmas0, lemmas1, method):\n",
    "    mean_sim0 = mean_simimilarity(lemmas0, lemmas1, method)\n",
    "    mean_sim1 = mean_simimilarity(lemmas1, lemmas0, method)\n",
    "\n",
    "    if mean_sim0 > 0 or mean_sim1 > 0:\n",
    "        return (2 * mean_sim0 * mean_sim1) / (mean_sim0 + mean_sim0)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lesk similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lesk_similarity(words0, words1):\n",
    "    w0_pos = nltk.pos_tag(words0)\n",
    "    w1_pos = nltk.pos_tag(words1)\n",
    "\n",
    "    s0_lesk = []\n",
    "    for i in range(len(w0_pos)):\n",
    "        if w0_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w0_pos[i][1][0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=w0_pos[i][1][0].lower()))\n",
    "\n",
    "    s1_lesk = []\n",
    "    for i in range(len(w1_pos)):\n",
    "        if w1_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w1_pos[i][1][0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=w1_pos[i][1][0].lower()))\n",
    "\n",
    "    return 1 - jaccard_distance(set(s0_lesk), set(s1_lesk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Methods(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration with the available similarity methods\n",
    "    \"\"\"\n",
    "    JACCARD = 'jaccard'\n",
    "    NE = 'NE'\n",
    "    PATH = 'path'\n",
    "    LEACOCK = 'lch'\n",
    "    WU = 'wup'\n",
    "    LIN = 'lin'\n",
    "    LESK = 'lesk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_similarity(sentence_0, sentence_1, method='jaccard'):\n",
    "    words0 = tokenize(sentence_0)\n",
    "    words1 = tokenize(sentence_1)\n",
    "    s0_lemmas = lemmatize_sentence(words0)\n",
    "    s1_lemmas = lemmatize_sentence(words1)\n",
    "\n",
    "    if method == 'jaccard':\n",
    "        return 1 - jaccard_distance(set(s0_lemmas), set(s1_lemmas))\n",
    "\n",
    "    elif method == 'NE':\n",
    "        s0_ne = name_entity_tokenization(sentence_0)\n",
    "        s1_ne = name_entity_tokenization(sentence_1)\n",
    "        return 1 - jaccard_distance(set(s0_ne), set(s1_ne))\n",
    "\n",
    "    elif method in ['path', 'lch', 'wup', 'lin']:\n",
    "        return synset_similarity(s0_lemmas, s1_lemmas, method)\n",
    "\n",
    "    elif method == 'lesk':\n",
    "        return lesk_similarity(words0, words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train\\STS.input.MSRpar.txt\n",
      "data\\train\\STS.input.MSRvid.txt\n",
      "data\\train\\STS.input.SMTeuroparl.txt\n",
      "data\\train\\STS.gs.MSRpar.txt\n",
      "data\\train\\STS.gs.MSRvid.txt\n",
      "data\\train\\STS.gs.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.input.MSRpar.txt\n",
      "data\\test-gold\\STS.input.MSRvid.txt\n",
      "data\\test-gold\\STS.input.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.input.surprise.OnWN.txt\n",
      "data\\test-gold\\STS.input.surprise.SMTnews.txt\n",
      "data\\test-gold\\STS.gs.ALL.txt\n",
      "data\\test-gold\\STS.gs.MSRpar.txt\n",
      "data\\test-gold\\STS.gs.MSRvid.txt\n",
      "data\\test-gold\\STS.gs.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.gs.surprise.OnWN.txt\n",
      "data\\test-gold\\STS.gs.surprise.SMTnews.txt\n"
     ]
    }
   ],
   "source": [
    "#path = os.path.join('data', 'train', 'STS.input.MSRpar.txt')\n",
    "\n",
    "#df_input = read_file(path)\n",
    "#df_input.head()\n",
    "\n",
    "\n",
    "#Train data\n",
    "inputTexts = []\n",
    "dataPath = os.path.join('data', 'train')\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        inputTexts.append(read_file(os.path.join(dataPath, filename)))\n",
    "\n",
    "inputGs = []\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.gs\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        inputGs.append(read_file(os.path.join(dataPath, filename)))\n",
    "\n",
    "##Test data\n",
    "inputTexts = []\n",
    "dataPath = os.path.join('data', 'test-gold')\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        inputTexts.append(read_file(os.path.join(dataPath, filename)))\n",
    "\n",
    "inputGs = []\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.gs\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        inputGs.append(read_file(os.path.join(dataPath, filename)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem likely will mean corrective changes before the shuttle fleet starts flying again.\n",
      "He said the problem needs to be corrected before the space shuttle fleet is cleared to fly again.\n",
      "Similarity using jaccard: 0.2857142857142857\n",
      "Similarity using NE: 0.28\n",
      "Similarity using path: 0.6518518518518519\n",
      "Similarity using lch: 0.8729193707472305\n",
      "Similarity using wup: 0.8361314611314613\n",
      "Similarity using lin: 0.810531749216512\n",
      "Similarity using lesk: 0.0625\n"
     ]
    }
   ],
   "source": [
    "# TODO: Remove testing cell\n",
    "s0 = inputTexts[0].iloc[0,0]\n",
    "s1 = inputTexts[0].iloc[0,1]\n",
    "print(s0)\n",
    "print(s1)\n",
    "for method in Methods:\n",
    "    print(f'Similarity using {method.value}: {compute_similarity(s0, s1, method.value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF and Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_tf(s0,s1):\n",
    "    #print (s0)\n",
    "    #print (s1)\n",
    "    s0,s1= expand_contractions(s0,s1)\n",
    "    s0 = nltk.word_tokenize(s0)\n",
    "    s1 = nltk.word_tokenize(s1)\n",
    "    tfvec=TfidfVectorizer()\n",
    "   \n",
    "\n",
    "\n",
    "    s0= lemmatize_sentence(s0)\n",
    "    s1 =lemmatize_sentence(s1)\n",
    "    # Generate the tf-idf vectors for the corpus\n",
    "    s0=' '.join([str(elem) for elem in s0])\n",
    "    s1=' '.join([str(elem) for elem in s1])\n",
    "    \n",
    "    #print(s0)\n",
    "    #print(s1)\n",
    "    tfidf_matrix = tfvec.fit_transform([s0,s1])\n",
    "\n",
    "    print(cosine_similarity(tfidf_matrix, tfidf_matrix)[0,1])\n",
    "    return cosine_similarity(tfidf_matrix, tfidf_matrix)[0,1]\n",
    "\n",
    "\n",
    "inputTexts=[]\n",
    "dataPath=os.path.join('data', 'train')\n",
    "x=[]\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print (os.path.join(dataPath,filename))\n",
    "        file=os.path.join(dataPath,filename)\n",
    "        text=pd.read_csv(file, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None, quoting=csv.QUOTE_NONE)   \n",
    "        for i in range(len(text['sentence0'])):\n",
    "            x.append(compute_tf(text['sentence0'][i],text['sentence1'][i]))\n",
    "        inputTexts.append(text)\n",
    "print (\"avg:\")\n",
    "print (sum(x)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct_and_lower(s0, s1):\n",
    "    s0 = [w.lower() for w in s0 if not all(c in punct for c in w)]\n",
    "    s1 = [w.lower() for w in s1 if not all(c in punct for c in w)]\n",
    "    return s0, s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train\\STS.input.MSRpar.txt\n",
      "data\\train\\STS.input.MSRvid.txt\n",
      "data\\train\\STS.input.SMTeuroparl.txt\n",
      "0.40481242682102353\n",
      "0.24857855371767784\n",
      "0.270118085631828\n",
      "0.15947397330965005\n",
      "0.18067483308917004\n",
      "0.10481912774434081\n"
     ]
    }
   ],
   "source": [
    "def compute_N_grams(s0,s1): \n",
    "  #words1=[word for word in s0.split(\" \") if word not in set(stopwords.words('english'))]  \n",
    "  lowSize=5\n",
    "  if len(s1)<len(s0):\n",
    "    s0,s1 = s1,s0\n",
    "  if len(s0)<5:\n",
    "    lowSize=len(s0)\n",
    "  s0,s1= expand_contractions(s0,s1)\n",
    "  s0 = nltk.word_tokenize(s0)\n",
    "  s1 = nltk.word_tokenize(s1)\n",
    "  s0,s1 = remove_punct_and_lower(s0,s1)\n",
    "  s0= lemmatize_sentence(s0)\n",
    "  s1 =lemmatize_sentence(s1)\n",
    "  ngrams=0\n",
    "  #print (s0)\n",
    "  #print (s1)\n",
    "  #print(\"Sentence after removing stopwords:\",s0)\n",
    "  metrics=[0,0,0,0,0,0]\n",
    "  for i in range(2,lowSize):\n",
    "    nGrams1=zip(*[s0[i:] for i in range(0,i)])\n",
    "    nGrams2=zip(*[s1[i:] for i in range(0,i)])\n",
    "    count=0\n",
    "\n",
    "    nGrams1=[' '.join(ngram) for ngram in nGrams1]\n",
    "    nGrams2=[' '.join(ngram) for ngram in nGrams2]\n",
    "    #print (set(nGrams2))\n",
    "    #print (set(nGrams1))\n",
    "    for j in set(nGrams1):\n",
    "      if j in set(nGrams2):\n",
    "        count+=1\n",
    "    if ((len(nGrams1) != 0) and (len(nGrams2) != 0)):\n",
    "      #print(1-jaccard_distance(set(nGrams1), set(nGrams2)))\n",
    "      #print (count)\n",
    "      metrics[(i-2)*2]=count/len(set(nGrams1))\n",
    "      metrics[((i-2)*2)+1]=1-jaccard_distance(set(nGrams1), set(nGrams2))\n",
    "  return metrics\n",
    "    \n",
    "\n",
    "\n",
    "inputTexts=[]\n",
    "dataPath=os.path.join('data', 'train')\n",
    "x=[]\n",
    "for filename in os.listdir(dataPath):\n",
    "  if \"STS.input\" in filename:\n",
    "    print (os.path.join(dataPath,filename))\n",
    "    file=os.path.join(dataPath,filename)\n",
    "    text=pd.read_csv(file, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None, quoting=csv.QUOTE_NONE)   \n",
    "    for i in range(len(text['sentence0'])):\n",
    "      metric=compute_N_grams(text['sentence0'][i],text['sentence1'][i])\n",
    "      x.append(metric)\n",
    "    inputTexts.append(text)\n",
    "#print (\"avg:\")\n",
    "numSen=len(x)\n",
    "x = np.array(x)\n",
    "print (sum(x[:,0])/numSen)\n",
    "print (sum(x[:,1])/numSen)\n",
    "print (sum(x[:,2])/numSen)\n",
    "print (sum(x[:,3])/numSen)\n",
    "print (sum(x[:,4])/numSen)\n",
    "print (sum(x[:,5])/numSen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-3501ee0b569f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Train the model on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Use the forest's predict method on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators=6, random_state=42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "regr = svm.SVR()\n",
    "regr.fit(X, y)\n",
    "SVR()\n",
    "regr.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsRegressor(n_neighbors=K)\n",
    "\n",
    "model.fit(x_train, y_train)  #fit the model\n",
    "pred = model.predict(x_test)  #make prediction on test set"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
