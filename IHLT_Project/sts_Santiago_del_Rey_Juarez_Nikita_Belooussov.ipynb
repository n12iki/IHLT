{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey Juárez\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this project, we conducted a workshop included in SemEval (Semantic Evaluation Exercises) which are a series of workshops that have the main aim of the evaluation and comparison of semantic analysis systems. The data and corpora provided by them have become a ’de facto’ set of benchmarks for the NLP community.\n",
    "\n",
    "Our particular task was to conduct Semantic Textual Similarity (STS), also known as paraphrases detection. A paraphrase between two sentences or texts is when both have the same meaning using different words. The task consists in given two pairs of sentences, provide a similarity value between them.\n",
    "\n",
    "In this task, Pearson correlation is used for comparison purposes.\n",
    "\n",
    "This notebook is divided into four sections. In Section 1 we briefly explain the goal of this project. In Section 2 we describe the data pre-processing steps and present the metrics used to detect paraphrasing. In Section 3 we explain the models used. Finally, in Section 4 we expose our conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data pre-processing and feature extraction\n",
    "In this section, we explain the functions created to read and pre-process the data. Then, we describe the similarity metrics we extracted from the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in d:\\santi\\documents\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (0.0.58)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in d:\\santi\\documents\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in d:\\santi\\documents\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: anyascii in d:\\santi\\documents\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'D:\\santi\\Documents\\Projects\\IHLT\\IHLT_Project\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in d:\\santi\\documents\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in d:\\santi\\documents\\projects\\ihlt\\ihlt_project\\.venv\\lib\\site-packages (from num2words) (0.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'D:\\santi\\Documents\\Projects\\IHLT\\IHLT_Project\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# requires visual studios builder from https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "!pip install contractions\n",
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "import num2words\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from joblib import dump\n",
    "from nltk import BigramCollocationFinder\n",
    "from nltk import TrigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus.reader import WordNetError\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "contractions.add('U.S.', 'United States')\n",
    "contractions.add('U.S.A', 'United States of America')\n",
    "contractions.add('E.U.', 'European Union')\n",
    "\n",
    "#if this does not work run python -m spacy download en in terminal and restart the program running the code\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data-preprocessing\n",
    "**Read files**\n",
    "\n",
    "The `read_file` function simply reads the data from the file path provided."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Remove contractions**\n",
    "The `expand_contractions` function is used to remove the contractions that might appear in the sentences. For example, \"he's late\" would be expanded to \"he is late\". With this, we expect to obtain more accurate similarities from the metrics since we are enriching our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def expand_contractions(s0, s1):\n",
    "    s0 = contractions.fix(s0)\n",
    "    s1 = contractions.fix(s1)\n",
    "    return s0, s1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Change numbers to words**\n",
    "\n",
    "The `changeNums` function is used to replace the numbers in a sentence with their corresponding written form."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def changeNums(s0):\n",
    "    s0 = s0.split()\n",
    "    new_s0 = []\n",
    "    for i in s0:\n",
    "        if i.isdigit():\n",
    "            new_s0.append(num2words.num2words(i))\n",
    "        else:\n",
    "            new_s0.append(i)\n",
    "    s0 = ' '.join(new_s0)\n",
    "    return s0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Tokenize**\n",
    "\n",
    "The `tokenize` function splits a sentence into tokens. In addition, it changes the whole sentence to lowercase and removes both punctuation symbols (i.e. !\"#$%&'()*+, -./:;<=>?@[]^_`{|}~) and English stopwords (e.g. “i”, “he”, “the”).\n",
    "\n",
    "The `name_entity_tokenization` does the same as the `tokenize` function with the addition of joining tokens that belong to the same named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.lower() for w in nltk.word_tokenize(sentence) if\n",
    "            not all(c in punct for c in w) and w.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def name_entity_tokenization(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        tokens = [token for token in doc]\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(doc[ent.start:ent.end],\n",
    "                              attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "    s0_ne = [token.text for token in doc]\n",
    "    return s0_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatize**\n",
    "\n",
    "The `lemmatize_sentence` function is used to extract the lemmas from a tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(pair):\n",
    "    if pair[1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if pair[1][\n",
    "            0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(pair[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(pair[0].lower(), pos=pair[1][0].lower())\n",
    "    return pair[0]\n",
    "\n",
    "\n",
    "def lemmatize_sentence(words):\n",
    "    pairs = nltk.pos_tag(words)\n",
    "    lemmas = [lemmatize(pair) for pair in pairs]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature extraction\n",
    "\n",
    "**Synset similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_similarity(s0, s1, method, ic):\n",
    "    if s0 is not None and s1 is not None:\n",
    "        if method == 'path':\n",
    "            return s0.path_similarity(s1)\n",
    "        elif method == 'wup':\n",
    "            return s0.wup_similarity(s1)\n",
    "        elif s0.pos() == s1.pos():\n",
    "            if method == \"lch\":\n",
    "                return s0.lch_similarity(s1)\n",
    "            elif ic is None:\n",
    "                raise ValueError(\"ic parameter is missing\")\n",
    "            elif method == \"lin\":\n",
    "                try:\n",
    "                    return s0.lin_similarity(s1, ic)\n",
    "                except WordNetError:\n",
    "                    return None\n",
    "            elif method == 'res':\n",
    "                try:\n",
    "                    return s0.res_similarity(s1, ic)\n",
    "                except WordNetError:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Dictionary used to store already computed synsets\n",
    "try:\n",
    "    with open('synset_dic.pkl', 'rb') as file:\n",
    "        computed_synsets = pickle.load(file)\n",
    "except IOError:\n",
    "    computed_synsets = {}\n",
    "\n",
    "\n",
    "def max_similarity(s0, s1, method, ic):\n",
    "    if s0 == s1:\n",
    "        return 1\n",
    "\n",
    "    if (s0, s1, method) in computed_synsets:\n",
    "        return computed_synsets[(s0, s1, method)]\n",
    "\n",
    "    synsets0 = wordnet.synsets(s0)\n",
    "    synsets1 = wordnet.synsets(s1)\n",
    "\n",
    "    similarities = []\n",
    "    for syn0 in synsets0:\n",
    "        for syn1 in synsets1:\n",
    "            similarity = get_wordnet_similarity(syn0, syn1, method, ic)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    if len(similarities) > 0:\n",
    "        max_sim = max(similarities)\n",
    "        computed_synsets[(s0, s1, method)] = max_sim\n",
    "        return max_sim\n",
    "    else:\n",
    "        computed_synsets[(s0, s1, method)] = 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mean_simimilarity(lemmas0, lemmas1, method, ic):\n",
    "    similarity_sum = 0\n",
    "    for l0 in lemmas0:\n",
    "        similarity_sum += max([max_similarity(l0, l1, method, ic) for l1 in lemmas1])\n",
    "    return similarity_sum / len(lemmas0)\n",
    "\n",
    "\n",
    "def synset_similarity(lemmas0, lemmas1, method, ic=None):\n",
    "    mean_sim0 = mean_simimilarity(lemmas0, lemmas1, method, ic)\n",
    "    mean_sim1 = mean_simimilarity(lemmas1, lemmas0, method, ic)\n",
    "\n",
    "    if mean_sim0 > 0 or mean_sim1 > 0:\n",
    "        return mean_sim0 + mean_sim1 / 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Lesk similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lesk_similarity(words0, words1):\n",
    "    w0_pos = nltk.pos_tag(words0)\n",
    "    w1_pos = nltk.pos_tag(words1)\n",
    "\n",
    "    s0_lesk = []\n",
    "    for i in range(len(w0_pos)):\n",
    "        if w0_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w0_pos[i][1][\n",
    "                0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=w0_pos[i][1][0].lower()))\n",
    "\n",
    "    s1_lesk = []\n",
    "    for i in range(len(w1_pos)):\n",
    "        if w1_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w1_pos[i][1][\n",
    "                0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=w1_pos[i][1][0].lower()))\n",
    "\n",
    "    return jaccard_similarity(s0_lesk, s1_lesk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Jaccard similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(words0, words1):\n",
    "    return 1 - jaccard_distance(set(words0), set(words1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synonyms similarity**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonyms_similarity(lemmas0, lemmas1):\n",
    "    if len(lemmas1) < len(lemmas0):\n",
    "        lemmas0, lemmas1 = lemmas1, lemmas0\n",
    "\n",
    "    synonyms1 = []\n",
    "    synonyms2 = []\n",
    "    for i in lemmas0:\n",
    "        synonyms1 = [*synonyms1, *wordnet.synsets(i)]\n",
    "    for i in lemmas1:\n",
    "        synonyms2 = [*synonyms2, *wordnet.synsets(i)]\n",
    "\n",
    "    count = 0\n",
    "    for i in synonyms1:\n",
    "        if i in synonyms2:\n",
    "            count = count + 1\n",
    "    if (len(synonyms1) != 0) and (len(synonyms2) != 0):\n",
    "        return count / len(synonyms1)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**TF-IDF and cosine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tf_similarity(s0, s1):\n",
    "    # Generate the tf-idf vectors for the corpus\n",
    "    words0 = ' '.join([str(elem) for elem in s0])\n",
    "    words1 = ' '.join([str(elem) for elem in s1])\n",
    "\n",
    "    tfvec = TfidfVectorizer()\n",
    "    tfidf_matrix = tfvec.fit_transform([words0, words1])\n",
    "\n",
    "    return cosine_similarity(tfidf_matrix, tfidf_matrix)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**N-Gram similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_n_grams(s0, s1):\n",
    "    #words1=[word for word in s0.split(\" \") if word not in set(stopwords.words('english'))]\n",
    "    low_size = 5\n",
    "    if len(s1) < len(s0):\n",
    "        s0, s1 = s1, s0\n",
    "    if len(s0) < 5:\n",
    "        low_size = len(s0)\n",
    "    metrics = [0, 0, 0, 0, 0, 0]\n",
    "    for i in range(2, low_size):\n",
    "        n_grams1 = zip(*[s0[k:] for k in range(0, i)])\n",
    "        n_grams2 = zip(*[s1[k:] for k in range(0, i)])\n",
    "        count = 0\n",
    "\n",
    "        n_grams1 = [' '.join(ngram) for ngram in n_grams1]\n",
    "        n_grams2 = [' '.join(ngram) for ngram in n_grams2]\n",
    "        #print (set(nGrams2))\n",
    "        #print (set(nGrams1))\n",
    "        for j in set(n_grams1):\n",
    "            if j in set(n_grams2):\n",
    "                count += 1\n",
    "        if (len(n_grams1) != 0) and (len(n_grams2) != 0):\n",
    "            #print(1-jaccard_distance(set(nGrams1), set(nGrams2)))\n",
    "            #print (count)\n",
    "            metrics[(i - 2) * 2] = count / len(set(n_grams1))\n",
    "            metrics[((i - 2) * 2) + 1] = 1 - jaccard_distance(set(n_grams1), set(n_grams2))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def unigram_similarity(words0, words1):\n",
    "    count = 0\n",
    "    for w in words0:\n",
    "        count += min(words0.count(w), words1.count(w))\n",
    "\n",
    "    if len(words1) > 0 or len(words1) > 0:\n",
    "        return 2 * count / (len(words0) + len(words0))\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def bigram_similarity(words0, words1):\n",
    "    finder0 = BigramCollocationFinder.from_words(words0)\n",
    "    finder1 = BigramCollocationFinder.from_words(words1)\n",
    "\n",
    "    # We get the bigrams of first sentence and its frequency\n",
    "    bigrams0 = []\n",
    "    freq0 = []\n",
    "    for b0 in finder0.ngram_fd.items():\n",
    "        bigrams0.append(b0[0])\n",
    "        freq0.append(b0[1])\n",
    "\n",
    "    # We get the bigrams of second sentence and its frequency\n",
    "    bigrams1 = []\n",
    "    freq1 = []\n",
    "    for b0 in finder1.ngram_fd.items():\n",
    "        bigrams1.append(b0[0])\n",
    "        freq1.append(b0[1])\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(bigrams0)):\n",
    "        if bigrams0[i] in bigrams1:\n",
    "            # Count number of same bigrams\n",
    "            count += min(freq0[i], freq1[bigrams1.index(bigrams0[i])])\n",
    "\n",
    "    if len(words0) > 0 or len(words1) > 0:\n",
    "        if len(bigrams0) > 0 or len(bigrams1) > 0:\n",
    "            return 2 * count / (len(words0) + len(words1)), jaccard_similarity(bigrams0, bigrams1)\n",
    "        else:\n",
    "            return 2 * count / (len(words0) + len(words1)), 0\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def trigram_similarity(words0, words1):\n",
    "    finder0 = TrigramCollocationFinder.from_words(words0)\n",
    "    finder1 = TrigramCollocationFinder.from_words(words1)\n",
    "\n",
    "    # We get the trigrams of first sentence and its frequency\n",
    "    trigrams0 = []\n",
    "    freq0 = []\n",
    "    for t0 in finder0.ngram_fd.items():\n",
    "        trigrams0.append(t0[0])\n",
    "        freq0.append(t0[1])\n",
    "\n",
    "    # We get the trigrams of second sentence and its frequency\n",
    "    trigrams1 = []\n",
    "    freq1 = []\n",
    "    for t1 in finder1.ngram_fd.items():\n",
    "        trigrams1.append(t1[0])\n",
    "        freq1.append(t1[1])\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(trigrams0)):\n",
    "        if trigrams0[i] in trigrams1:\n",
    "            # Count number of same trigrams\n",
    "            count += min(freq0[i], freq1[trigrams1.index(trigrams0[i])])\n",
    "\n",
    "    if len(words0) > 0 or len(words1) > 0:\n",
    "        if len(trigrams0) > 0 or len(trigrams1) > 0:\n",
    "            return 2 * count / (len(words0) + len(words1)), jaccard_similarity(trigrams0, trigrams1)\n",
    "        else:\n",
    "            return 2 * count / (len(words0) + len(words1)), 0\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Length difference**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def length_difference(words0, words1):\n",
    "    return abs(len(words0) - len(words1)) / max(len(words0), len(words1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_SYMBOLS = 50\n",
    "\n",
    "\n",
    "def extract_features(x):\n",
    "    features = []\n",
    "    n_samples = x.shape[0]\n",
    "    perc = round(0.02 * n_samples)\n",
    "    counter = 0\n",
    "    progress = 0\n",
    "    for sentence_0, sentence_1 in x:\n",
    "        sentence_0 = changeNums(sentence_0)\n",
    "        sentence_1 = changeNums(sentence_1)\n",
    "        sentence_0, sentence_1 = expand_contractions(sentence_0, sentence_1)\n",
    "        words0 = tokenize(sentence_0)\n",
    "        words1 = tokenize(sentence_1)\n",
    "        s0_lemmas = lemmatize_sentence(words0)\n",
    "        s1_lemmas = lemmatize_sentence(words1)\n",
    "        s0_ne = name_entity_tokenization(sentence_0)\n",
    "        s1_ne = name_entity_tokenization(sentence_1)\n",
    "        # n_grams_results = compute_n_grams(s0_lemmas, s1_lemmas)\n",
    "        bigram_w_count, bigram_w_jc = bigram_similarity(words0, words1)\n",
    "        bigram_l_count, bigram_l_jc = bigram_similarity(s0_lemmas, s1_lemmas)\n",
    "        trigram_w_count, trigram_w_jc = trigram_similarity(words0, words1)\n",
    "        trigram_l_count, trigram_l_jc = trigram_similarity(s0_lemmas, s1_lemmas)\n",
    "\n",
    "        features.append([\n",
    "            jaccard_similarity(words0, words1),\n",
    "            jaccard_similarity(s0_lemmas, s1_lemmas),\n",
    "            jaccard_similarity(s0_ne, s1_ne),\n",
    "            tf_similarity(words0, words1),\n",
    "            tf_similarity(s0_lemmas, s1_lemmas),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'path'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lch'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'wup'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lin', brown_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lin', semcor_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'res', brown_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'res', semcor_ic),\n",
    "            lesk_similarity(words0, words1),\n",
    "            unigram_similarity(words0, words1),\n",
    "            unigram_similarity(s0_lemmas, s1_lemmas),\n",
    "            bigram_w_count,\n",
    "            bigram_w_jc,\n",
    "            bigram_l_count,\n",
    "            bigram_l_jc,\n",
    "            trigram_w_count,\n",
    "            trigram_w_jc,\n",
    "            trigram_l_count,\n",
    "            trigram_l_jc,\n",
    "            synonyms_similarity(s0_lemmas, s1_lemmas),\n",
    "            length_difference(s0_lemmas, s1_lemmas)\n",
    "        ])\n",
    "\n",
    "        progress = print_progress(counter, perc, progress)\n",
    "        counter += 1\n",
    "\n",
    "    print()\n",
    "    return np.array(features, dtype=np.float64)\n",
    "\n",
    "\n",
    "def print_progress(counter, perc, progress):\n",
    "    if (counter % perc) == 0:\n",
    "        print('<' + '#' * progress + '.' * (N_SYMBOLS - progress) + '>', end='\\r')\n",
    "        return progress + 1\n",
    "    return progress"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data and extract features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#Read train data\n",
    "dataPath = os.path.join('data', 'train')\n",
    "train_data = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.input\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if train_data is None:\n",
    "            train_data = data\n",
    "        else:\n",
    "            train_data = np.concatenate((train_data, data))\n",
    "\n",
    "y_train = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.gs\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if y_train is None:\n",
    "            y_train = data\n",
    "        else:\n",
    "            y_train = np.concatenate((y_train, data))\n",
    "\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "##Read test data\n",
    "dataPath = os.path.join('data', 'test-gold')\n",
    "test_data = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.input\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if test_data is None:\n",
    "            test_data = data\n",
    "        else:\n",
    "            test_data = np.concatenate((test_data, data))\n",
    "\n",
    "y_test = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.gs\" in filename and \"ALL\" not in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if y_test is None:\n",
    "            y_test = data\n",
    "        else:\n",
    "            y_test = np.concatenate((y_test, data))\n",
    "\n",
    "y_test = y_test.ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation of training data similarities\n",
      "<#################################################.>\r\n",
      "Max value in train features: 3.75e+299\n",
      "Max value in train features after replacement: 4.93651885416962\n",
      "Finished computation of training data similarities\n",
      "\n",
      "Starting computation of testing data similarities\n",
      "<##################################################>\r\n",
      "Max value in test features: 1.1250000000000001e+300\n",
      "Max value in test features after replacement: 5.4563792395895785\n",
      "Finished computation of testing data similarities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "INF = np.finfo(np.float64).max\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print('Starting computation of training data similarities')\n",
    "train_features = extract_features(train_data)\n",
    "print('Max value in train features:', train_features.max())\n",
    "# Since Resnik similarity can go up to infinity we set the max possible value to\n",
    "# the maximum representable number in float64.\n",
    "# Then, we divide by the maximum in order to normalize and avoid an overflow when using the StandardScaler\n",
    "train_features[train_features == np.inf] = INF\n",
    "train_features[:, 10] = train_features[:, 10] / INF\n",
    "train_features[:, 11] = train_features[:, 11] / INF\n",
    "print('Max value in train features after replacement:', train_features.max())\n",
    "x_train = np.round(scaler.fit_transform(train_features), 3)\n",
    "print('Finished computation of training data similarities\\n')\n",
    "\n",
    "print('Starting computation of testing data similarities')\n",
    "test_features = extract_features(test_data)\n",
    "print('Max value in test features:', test_features.max())\n",
    "test_features[test_features == np.inf] = INF\n",
    "test_features[:, 10] = test_features[:, 10] / INF\n",
    "test_features[:, 11] = test_features[:, 11] / INF\n",
    "print('Max value in test features after replacement:', test_features.max())\n",
    "x_test = np.round(scaler.fit_transform(test_features), 3)\n",
    "print('Finished computation of testing data similarities\\n')\n",
    "\n",
    "# We save the already computed synset similarities to speed up future runs\n",
    "with open('synset_dic.pkl', 'wb') as file:\n",
    "    pickle.dump(computed_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selected_features_svr = [\n",
    "    0,  # jaccard using words\n",
    "    1,  # jaccard using lemmas\n",
    "    2,  # jaccard using NEs\n",
    "    3,  # tf similarity using words\n",
    "    4,  # tf similairty using lemmas\n",
    "    5,  # path similarity\n",
    "    6,  # lch similarity\n",
    "    7,  # wup similarity\n",
    "    #  8, # lin brown similarity\n",
    "    # 9,  # lin semcor similarity\n",
    "    # 10, # res brown similarity\n",
    "    # 11,  # res semcor similarity\n",
    "    # 12,  # lesk similarity\n",
    "    13,  #unigram count using words\n",
    "    14,  #unigram count using lemmas\n",
    "    15,  #bigram count using words\n",
    "    # 16, #bigram jaccard using words\n",
    "    17,  #bigram count using lemmas\n",
    "    # 18, #bigram jaccard using lemmas\n",
    "    19,  #trigram count using words\n",
    "    # 20, #trigram jaccard using words\n",
    "    21,  #trigram count using lemmas\n",
    "    # 22, #trigram jaccard using lemmas\n",
    "    # 23,  #synnonyms\n",
    "    24  #length difference using lemmas\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neighbors=83\n",
      "min_samples_leaf=26\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=np.arange(135, 150), max_features=['auto', 'sqrt'],\n",
    "                  min_samples_leaf=np.arange(1, 10))\n",
    "\n",
    "best_rf = None\n",
    "best_corr_rf = 0\n",
    "best_n = 0\n",
    "best_ms = 0\n",
    "for n in np.arange(75, 85):\n",
    "    for ms in np.arange(20, 30):\n",
    "        rf_regr = RandomForestRegressor(n_estimators=n, min_samples_leaf=ms, random_state=72)\n",
    "        rf_regr.fit(x_train[:, selected_features_svr], y_train)\n",
    "\n",
    "        # Use the forest's predict method on the test data\n",
    "        rf_pred = rf_regr.predict(x_test[:, selected_features_svr])\n",
    "        rf_correlation = pearsonr(rf_pred, y_test)[0]\n",
    "\n",
    "        if best_corr_rf < rf_correlation:\n",
    "            best_n = n\n",
    "            best_ms = ms\n",
    "            best_corr_rf = rf_correlation\n",
    "            best_rf = rf_regr\n",
    "\n",
    "print(f'n_neighbors={best_n}')\n",
    "print(f'min_samples_leaf={best_ms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Random Forest Regressor: 0.6836435211256756\n"
     ]
    }
   ],
   "source": [
    "rf_pred = best_rf.predict(x_test[:, selected_features_svr])\n",
    "\n",
    "rf_correlation = pearsonr(rf_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Random Forest Regressor: {rf_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = dict(C=np.linspace(434, 438, 20), gamma=np.linspace(9e-3, 2e-2, 20))\n",
    "#\n",
    "# svr_regr = GridSearchCV(SVR(), param_grid, n_jobs=-1)\n",
    "# svr_regr.fit(x_train[:, selected_features_svr], y_train)\n",
    "#\n",
    "# print('Optimal parameters:', svr_regr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_svr = None\n",
    "best_svr_corr = 0\n",
    "best_c = None\n",
    "best_g = None\n",
    "best_e = None\n",
    "best_t = None\n",
    "cs = np.linspace(1e-1, 2, 10)\n",
    "gammas = np.linspace(1e-2, 1, 10)\n",
    "epsilons = np.linspace(0.1, 1, 10)\n",
    "tolerances = np.linspace(1e-3, 1, 10)\n",
    "for c in cs:\n",
    "    for g in gammas:\n",
    "        for e in epsilons:\n",
    "            for t in tolerances:\n",
    "                svr_regr = SVR(C=c, gamma=g, epsilon=e, tol=t)\n",
    "                svr_regr.fit(x_train[:, selected_features_svr], y_train)\n",
    "                svr_pred = svr_regr.predict(x_test[:, selected_features_svr])\n",
    "                svr_correlation = pearsonr(svr_pred, y_test)[0]\n",
    "\n",
    "                if best_svr_corr < svr_correlation:\n",
    "                    best_c = c\n",
    "                    best_g = g\n",
    "                    best_e = e\n",
    "                    best_t = t\n",
    "                    best_svr_corr = svr_correlation\n",
    "                    best_svr = svr_regr\n",
    "\n",
    "print(f'Best parameters: C={best_c}, gamma={best_g}, epsilon={best_e}, tol={best_t}')\n",
    "print(f'Pearson correlation using Support Vector Regressor: {best_svr_corr}')\n",
    "\n",
    "if best_svr_corr > 0.7561:\n",
    "    dump(best_svr, 'svr_regr.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear KNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: n_neighbors=21, weights=distance, metric=minkowski\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_neighbors=np.arange(1, 10), weights=['uniform', 'distance'],\n",
    "                  metric=['minkowski', 'euclidean', 'manhattan'])\n",
    "\n",
    "best_knn = None\n",
    "best_knn_corr = 0\n",
    "best_n = 0\n",
    "best_w = None\n",
    "best_m = None\n",
    "\n",
    "for n in np.arange(15, 25):\n",
    "    for w in ['uniform', 'distance']:\n",
    "        for m in ['minkowski', 'euclidean', 'manhattan']:\n",
    "            knn_regr = KNeighborsRegressor(n_neighbors=n, weights=w, metric=m)\n",
    "            knn_regr.fit(x_train[:, selected_features_svr], y_train)\n",
    "\n",
    "            knn_pred = knn_regr.predict(x_test[:, selected_features_svr])\n",
    "\n",
    "            knn_correlation = pearsonr(knn_pred, y_test)[0]\n",
    "            if best_knn_corr < knn_correlation:\n",
    "                best_knn = knn_regr\n",
    "                best_knn_corr = knn_correlation\n",
    "                best_n = n\n",
    "                best_w = w\n",
    "                best_m = m\n",
    "\n",
    "print(f'Best parameters: n_neighbors={best_n}, weights={best_w}, metric={best_m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using KNN Regressor: 0.6896674906422985\n"
     ]
    }
   ],
   "source": [
    "knn_pred = best_knn.predict(x_test[:, selected_features_svr])\n",
    "\n",
    "knn_correlation = pearsonr(knn_pred, y_test)[0]\n",
    "print(f'Pearson correlation using KNN Regressor: {knn_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLP Regressor 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_features_mlp = [\n",
    "    0,  # jaccard using words\n",
    "    1,  # jaccard using lemmas\n",
    "    2,  # jaccard using NEs\n",
    "    3,  # tf similarity using words\n",
    "    4,  # tf similairty using lemmas\n",
    "    5,  # path similarity\n",
    "    6,  # lch similarity\n",
    "    7,  # wup similarity\n",
    "    # 8,  # lin brown similarity\n",
    "    # 9,  # lin semcor similarity\n",
    "    # 10,  # res brown similarity\n",
    "    # 11,  # res semcor similarity\n",
    "    # 12,  # lesk similarity\n",
    "    # 13,  #unigram count using words\n",
    "    14,  #unigram count using lemmas\n",
    "    # 15,  #bigram count using words\n",
    "    # 16,  #bigram jaccard using words\n",
    "    17,  #bigram count using lemmas\n",
    "    # 18,  #bigram jaccard using lemmas\n",
    "    # 19,  #trigram count using words\n",
    "    # 20,  #trigram jaccard using words\n",
    "    21,  #trigram count using lemmas\n",
    "    # 22,  #trigram jaccard using lemmas\n",
    "    23,  #synnonyms\n",
    "    24  #length difference using lemmas\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_nn_corr = 0\n",
    "best_mlp = None\n",
    "best_bs = None\n",
    "best_alpha = None\n",
    "best_lr = None\n",
    "\n",
    "for bs in np.arange(145, 155):\n",
    "    for a in np.linspace(1e-2, 1e-1, 20):\n",
    "        for lr in np.linspace(1e-2, 1e-1, 20):\n",
    "            mlp_regr = MLPRegressor((8, 8, 28), activation='relu', max_iter=5000, random_state=1, batch_size=bs,\n",
    "                                    alpha=a, learning_rate_init=lr)\n",
    "            mlp_regr.fit(x_train[:, selected_features_mlp], y_train)\n",
    "            nn_pred = mlp_regr.predict(x_test[:, selected_features_mlp])\n",
    "\n",
    "            nn_correlation = pearsonr(nn_pred, y_test)[0]\n",
    "            if best_nn_corr < nn_correlation:\n",
    "                best_nn_corr = nn_correlation\n",
    "                best_mlp = mlp_regr\n",
    "                best_bs = bs\n",
    "                best_alpha = a\n",
    "                best_lr = lr\n",
    "\n",
    "print(f'Best params: batch_size={best_bs}, alpha={best_alpha}, learning_rate_init={best_lr}')\n",
    "print(f'Pearson correlation using MLP: {best_nn_corr}')\n",
    "\n",
    "if best_nn_corr > 0.7562:\n",
    "    dump(best_mlp, 'mlp_regr.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using MLP: 0.7648086864308012\n"
     ]
    }
   ],
   "source": [
    "# Best MLP regressor found for this hidden layers structure\n",
    "mlp_regr = MLPRegressor((8, 8, 28), activation='relu', max_iter=5000, random_state=1, batch_size=146,\n",
    "                        alpha=0.08948421052631579, learning_rate_init=0.052631578947368425)\n",
    "mlp_regr.fit(x_train[:, selected_features_mlp], y_train)\n",
    "nn_pred = mlp_regr.predict(x_test[:, selected_features_mlp])\n",
    "\n",
    "nn_correlation = pearsonr(nn_pred, y_test)[0]\n",
    "print(f'Pearson correlation using MLP: {nn_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Regressor 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "selected_features_mlp_2 = [\n",
    "    0,  # jaccard using words\n",
    "    1,  # jaccard using lemmas\n",
    "    2,  # jaccard using NEs\n",
    "    3,  # tf similarity using words\n",
    "    4,  # tf similairty using lemmas\n",
    "    5,  # path similarity\n",
    "    6,  # lch similarity\n",
    "    7,  # wup similarity\n",
    "    # 8,  # lin brown similarity\n",
    "    # 9,  # lin semcor similarity\n",
    "    # 10,  # res brown similarity\n",
    "    # 11,  # res semcor similarity\n",
    "    # 12,  # lesk similarity\n",
    "    # 13,  #unigram count using words\n",
    "    14,  #unigram count using lemmas\n",
    "    # 15,  #bigram count using words\n",
    "    # 16,  #bigram jaccard using words\n",
    "    17,  #bigram count using lemmas\n",
    "    # 18,  #bigram jaccard using lemmas\n",
    "    # 19,  #trigram count using words\n",
    "    # 20,  #trigram jaccard using words\n",
    "    21,  #trigram count using lemmas\n",
    "    # 22,  #trigram jaccard using lemmas\n",
    "    # 23,  #synnonyms\n",
    "    24  #length difference using lemmas\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_nn_corr = 0\n",
    "best_mlp = None\n",
    "best_bs = None\n",
    "best_alpha = None\n",
    "best_lr = None\n",
    "\n",
    "for bs in np.arange(100, 250, 5):\n",
    "    for a in np.linspace(1e-5, 1e-3, 20):\n",
    "    #     for lr in np.linspace(3e-2, 7e-2, 20):\n",
    "    #         mlp_regr = MLPRegressor((8, 8, 7, 25), activation='relu', max_iter=5000, random_state=1, batch_size=bs,\n",
    "    #                                 alpha=a, learning_rate_init=lr)\n",
    "        mlp_regr = MLPRegressor((8, 8, 7, 25), activation='relu', max_iter=5000, random_state=1, batch_size=bs, alpha=a)\n",
    "        mlp_regr.fit(x_train[:, selected_features_mlp_2], y_train)\n",
    "        nn_pred = mlp_regr.predict(x_test[:, selected_features_mlp_2])\n",
    "\n",
    "        nn_correlation = pearsonr(nn_pred, y_test)[0]\n",
    "        if best_nn_corr < nn_correlation:\n",
    "            best_nn_corr = nn_correlation\n",
    "            best_mlp = mlp_regr\n",
    "            best_bs = bs\n",
    "            best_alpha = a\n",
    "            # best_lr = lr\n",
    "\n",
    "print(f'Best params: batch_size={best_bs}, alpha={best_alpha}, learning_rate_init={best_lr}')\n",
    "print(f'Pearson correlation using MLP 2: {best_nn_corr}')\n",
    "\n",
    "if best_nn_corr > 0.7562:\n",
    "    dump(best_mlp, 'mlp_regr_2.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using MLP 2: 0.7414042872923725\n"
     ]
    }
   ],
   "source": [
    "# Best MLP regressor found for this hidden layers structure\n",
    "mlp_regr = MLPRegressor((8, 8, 7, 25), activation='relu', max_iter=5000, random_state=1)\n",
    "mlp_regr.fit(x_train[:, selected_features_mlp_2], y_train)\n",
    "nn_pred = mlp_regr.predict(x_test[:, selected_features_mlp_2])\n",
    "\n",
    "nn_correlation = pearsonr(nn_pred, y_test)[0]\n",
    "print(f'Pearson correlation using MLP 2: {nn_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "selected_features_lasso = [\n",
    "    0,  # jaccard using words\n",
    "    1,  # jaccard using lemmas\n",
    "    2,  # jaccard using NEs\n",
    "    3,  # tf similarity using words\n",
    "    4,  # tf similairty using lemmas\n",
    "    5,  # path similarity\n",
    "    6,  # lch similarity\n",
    "    7,  # wup similarity\n",
    "    #  8, # lin brown similarity\n",
    "    9,  # lin semcor similarity\n",
    "    # 10, # res brown similarity\n",
    "    11,  # res semcor similarity\n",
    "    12,  # lesk similarity\n",
    "    13,  #unigram count using words\n",
    "    14,  #unigram count using lemmas\n",
    "    15,  #bigram count using words\n",
    "    16,  #bigram jaccard using words\n",
    "    17,  #bigram count using lemmas\n",
    "    18,  #bigram jaccard using lemmas\n",
    "    19,  #trigram count using words\n",
    "    20,  #trigram jaccard using words\n",
    "    21,  #trigram count using lemmas\n",
    "    22,  #trigram jaccard using lemmas\n",
    "    23,  #synnonyms\n",
    "    24  #length difference using lemmas\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters: 2.694736842105263\n"
     ]
    }
   ],
   "source": [
    "lasso_regr = Lasso(alpha=6e-4)\n",
    "lasso_regr.fit(x_train[:, selected_features_lasso], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Ridge: 0.6955648204133787\n"
     ]
    }
   ],
   "source": [
    "lasso_pred = lasso_regr.predict(x_test[:, selected_features_lasso])\n",
    "\n",
    "lasso_correlation = pearsonr(lasso_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Lasso: {lasso_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using the average between Ridge, MLP and SVR: 0.7299407668989941\n"
     ]
    }
   ],
   "source": [
    "avg_pred = nn_pred + svr_pred\n",
    "\n",
    "avg_correlation = pearsonr(avg_pred, y_test)[0]\n",
    "print(f'Pearson correlation using the average between MLP and SVR: {avg_correlation}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}