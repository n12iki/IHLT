{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey Ju√°rez\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\santi\\Projects\\IHLT\\IHLT_Project\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# requires visual studios builder from https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "!pip install contractions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_56676/2242599445.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstring\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mcontractions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import string\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus.reader import WordNetError\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "contractions.add('U.S.', 'United States')\n",
    "contractions.add('U.S.A', 'United States of America')\n",
    "contractions.add('E.U.', 'European Union')\n",
    "\n",
    "#if this does not work run python -m spacy download en in terminal and restart the program running the code\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove contractions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def expand_contractions(s0,s1):\n",
    "    s0=contractions.fix(s0)\n",
    "    s1=contractions.fix(s1)\n",
    "    return s0,s1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.lower() for w in nltk.word_tokenize(sentence) if\n",
    "            not all(c in punct for c in w) and w.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def name_entity_tokenization(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        tokens = [token for token in doc]\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(doc[ent.start:ent.end],\n",
    "                              attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "    s0_ne = [token.text for token in doc]\n",
    "    return s0_ne"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(pair):\n",
    "    if pair[1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if pair[1][\n",
    "            0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(pair[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(pair[0].lower(), pos=pair[1][0].lower())\n",
    "    return pair[0]\n",
    "\n",
    "\n",
    "def lemmatize_sentence(words):\n",
    "    pairs = nltk.pos_tag(words)\n",
    "    lemmas = [lemmatize(pair) for pair in pairs]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Synset similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_similarity(s0, s1, method, ic):\n",
    "    if s0 is not None and s1 is not None:\n",
    "        if method == 'path':\n",
    "            return s0.path_similarity(s1)\n",
    "        elif method == 'wup':\n",
    "            return s0.wup_similarity(s1)\n",
    "        elif s0.pos() == s1.pos():\n",
    "            if method == \"lch\":\n",
    "                return s0.lch_similarity(s1)\n",
    "            elif ic is None:\n",
    "                raise ValueError(\"ic parameter is missing\")\n",
    "            elif method == \"lin\":\n",
    "                try:\n",
    "                    return s0.lin_similarity(s1, ic)\n",
    "                except WordNetError:\n",
    "                    return None\n",
    "            elif method == 'res':\n",
    "                try:\n",
    "                    return s0.res_similarity(s1, ic)\n",
    "                except WordNetError:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Dictionary used to store already computed synsets\n",
    "computed_synsets = {}\n",
    "\n",
    "\n",
    "def max_similarity(s0, s1, method, ic):\n",
    "    if s0 == s1:\n",
    "        return 1\n",
    "\n",
    "    if (s0, s1, method) in computed_synsets:\n",
    "        return computed_synsets[(s0, s1, method)]\n",
    "\n",
    "    synsets0 = wordnet.synsets(s0)\n",
    "    synsets1 = wordnet.synsets(s1)\n",
    "\n",
    "    similarities = []\n",
    "    for syn0 in synsets0:\n",
    "        for syn1 in synsets1:\n",
    "            similarity = get_wordnet_similarity(syn0, syn1, method, ic)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    if len(similarities) > 0:\n",
    "        max_sim = max(similarities)\n",
    "        computed_synsets[(s0, s1, method)] = max_sim\n",
    "        return max_sim\n",
    "    else:\n",
    "        computed_synsets[(s0, s1, method)] = 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mean_simimilarity(lemmas0, lemmas1, method, ic):\n",
    "    similarity_sum = 0\n",
    "    for l0 in lemmas0:\n",
    "        similarity_sum += max([max_similarity(l0, l1, method, ic) for l1 in lemmas1])\n",
    "    return similarity_sum / len(lemmas0)\n",
    "\n",
    "\n",
    "def synset_similarity(lemmas0, lemmas1, method, ic=None):\n",
    "    mean_sim0 = mean_simimilarity(lemmas0, lemmas1, method, ic)\n",
    "    mean_sim1 = mean_simimilarity(lemmas1, lemmas0, method, ic)\n",
    "\n",
    "    if mean_sim0 > 0 or mean_sim1 > 0:\n",
    "        return (2 * mean_sim0 * mean_sim1) / (mean_sim0 + mean_sim0)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lesk similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lesk_similarity(words0, words1):\n",
    "    w0_pos = nltk.pos_tag(words0)\n",
    "    w1_pos = nltk.pos_tag(words1)\n",
    "\n",
    "    s0_lesk = []\n",
    "    for i in range(len(w0_pos)):\n",
    "        if w0_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w0_pos[i][1][\n",
    "                0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=w0_pos[i][1][0].lower()))\n",
    "\n",
    "    s1_lesk = []\n",
    "    for i in range(len(w1_pos)):\n",
    "        if w1_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w1_pos[i][1][\n",
    "                0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=w1_pos[i][1][0].lower()))\n",
    "\n",
    "    return 1 - jaccard_distance(set(s0_lesk), set(s1_lesk))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Jaccard similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def jaccard_similarity(words0, words1):\n",
    "    return 1 - jaccard_distance(set(words0), set(words1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF IDF and cosine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_tf(s0,s1):\n",
    "    #print (s0)\n",
    "    #print (s1)\n",
    "    s0,s1= expand_contractions(s0,s1)\n",
    "    s0 = nltk.word_tokenize(s0)\n",
    "    s1 = nltk.word_tokenize(s1)\n",
    "    tfvec=TfidfVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "    s0= lemmatize_sentence(s0)\n",
    "    s1 =lemmatize_sentence(s1)\n",
    "    # Generate the tf-idf vectors for the corpus\n",
    "    s0=' '.join([str(elem) for elem in s0])\n",
    "    s1=' '.join([str(elem) for elem in s1])\n",
    "\n",
    "    #print(s0)\n",
    "    #print(s1)\n",
    "    tfidf_matrix = tfvec.fit_transform([s0,s1])\n",
    "\n",
    "    print(cosine_similarity(tfidf_matrix, tfidf_matrix)[0,1])\n",
    "    return cosine_similarity(tfidf_matrix, tfidf_matrix)[0,1]\n",
    "\n",
    "\n",
    "inputTexts=[]\n",
    "dataPath=os.path.join('data', 'train')\n",
    "x=[]\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print (os.path.join(dataPath,filename))\n",
    "        file=os.path.join(dataPath,filename)\n",
    "        text=pd.read_csv(file, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None, quoting=csv.QUOTE_NONE)\n",
    "        for i in range(len(text['sentence0'])):\n",
    "            x.append(compute_tf(text['sentence0'][i],text['sentence1'][i]))\n",
    "        inputTexts.append(text)\n",
    "print (\"avg:\")\n",
    "print (sum(x)/len(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### N-Gram similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_n_grams(s0, s1):\n",
    "    #words1=[word for word in s0.split(\" \") if word not in set(stopwords.words('english'))]\n",
    "    low_size=5\n",
    "    if len(s1)<len(s0):\n",
    "        s0,s1 = s1,s0\n",
    "    if len(s0)<5:\n",
    "        low_size=len(s0)\n",
    "    s0,s1= expand_contractions(s0,s1)\n",
    "    s0 = tokenize(s0)\n",
    "    s1 = tokenize(s1)\n",
    "    s0= lemmatize_sentence(s0)\n",
    "    s1 =lemmatize_sentence(s1)\n",
    "    ngrams=0\n",
    "    #print (s0)\n",
    "    #print (s1)\n",
    "    #print(\"Sentence after removing stopwords:\",s0)\n",
    "    metrics=[0,0,0,0,0,0]\n",
    "    for i in range(2,low_size):\n",
    "        n_grams1=zip(*[s0[k:] for k in range(0, i)])\n",
    "        n_grams2=zip(*[s1[k:] for k in range(0,i)])\n",
    "        count=0\n",
    "\n",
    "        n_grams1=[' '.join(ngram) for ngram in n_grams1]\n",
    "        n_grams2=[' '.join(ngram) for ngram in n_grams2]\n",
    "        #print (set(nGrams2))\n",
    "        #print (set(nGrams1))\n",
    "        for j in set(n_grams1):\n",
    "            if j in set(n_grams2):\n",
    "                count+=1\n",
    "        if (len(n_grams1) != 0) and (len(n_grams2) != 0):\n",
    "            #print(1-jaccard_distance(set(nGrams1), set(nGrams2)))\n",
    "            #print (count)\n",
    "            metrics[(i-2)*2]=count/len(set(n_grams1))\n",
    "            metrics[((i-2)*2)+1]=1-jaccard_distance(set(n_grams1), set(n_grams2))\n",
    "    return metrics\n",
    "\n",
    "inputTexts=[]\n",
    "dataPath=os.path.join('data', 'train')\n",
    "x=[]\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print (os.path.join(dataPath,filename))\n",
    "        file=os.path.join(dataPath,filename)\n",
    "        text=pd.read_csv(file, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None, quoting=csv.QUOTE_NONE)\n",
    "        for i in range(len(text['sentence0'])):\n",
    "            metric=compute_n_grams(text['sentence0'][i],text['sentence1'][i])\n",
    "            x.append(metric)\n",
    "        inputTexts.append(text)\n",
    "#print (\"avg:\")\n",
    "numSen=len(x)\n",
    "x = np.array(x)\n",
    "print (sum(x[:,0])/numSen)\n",
    "print (sum(x[:,1])/numSen)\n",
    "print (sum(x[:,2])/numSen)\n",
    "print (sum(x[:,3])/numSen)\n",
    "print (sum(x[:,4])/numSen)\n",
    "print (sum(x[:,5])/numSen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_SYMBOLS = 99\n",
    "\n",
    "\n",
    "def compute_similarity(x):\n",
    "    features = []\n",
    "    n_samples = x.shape[0]\n",
    "    perc = round(0.02 * n_samples)\n",
    "    counter = 0\n",
    "    progress = 0\n",
    "    for sentence_0, sentence_1 in x:\n",
    "        words0 = tokenize(sentence_0)\n",
    "        words1 = tokenize(sentence_1)\n",
    "        s0_lemmas = lemmatize_sentence(words0)\n",
    "        s1_lemmas = lemmatize_sentence(words1)\n",
    "        s0_ne = name_entity_tokenization(sentence_0)\n",
    "        s1_ne = name_entity_tokenization(sentence_1)\n",
    "\n",
    "        features.append([\n",
    "            jaccard_similarity(words0, words1),\n",
    "            jaccard_similarity(s0_lemmas, s1_lemmas),\n",
    "            jaccard_similarity(s0_ne, s1_ne),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'path'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lch'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'wup'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lin', brown_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lin', semcor_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'res', brown_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'res', semcor_ic),\n",
    "            lesk_similarity(words0, words1)\n",
    "        ])\n",
    "\n",
    "        progress = print_progress(counter, perc, progress)\n",
    "\n",
    "    print()\n",
    "    return np.array(features, dtype=np.float64)\n",
    "\n",
    "\n",
    "def print_progress(counter, perc, progress):\n",
    "    if (counter % perc) == 0:\n",
    "        print('<' + '#' * progress + '.' * (N_SYMBOLS - progress) + '>', end='\\r')\n",
    "        return progress + 1\n",
    "    return progress"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train\\STS.input.MSRpar.txt\n",
      "data\\train\\STS.input.MSRvid.txt\n",
      "data\\train\\STS.input.SMTeuroparl.txt\n",
      "data\\train\\STS.gs.MSRpar.txt\n",
      "data\\train\\STS.gs.MSRvid.txt\n",
      "data\\train\\STS.gs.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.input.MSRpar.txt\n",
      "data\\test-gold\\STS.input.MSRvid.txt\n",
      "data\\test-gold\\STS.input.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.input.surprise.OnWN.txt\n",
      "data\\test-gold\\STS.input.surprise.SMTnews.txt\n",
      "data\\test-gold\\STS.gs.ALL.txt\n",
      "data\\test-gold\\STS.gs.MSRpar.txt\n",
      "data\\test-gold\\STS.gs.MSRvid.txt\n",
      "data\\test-gold\\STS.gs.SMTeuroparl.txt\n",
      "data\\test-gold\\STS.gs.surprise.OnWN.txt\n",
      "data\\test-gold\\STS.gs.surprise.SMTnews.txt\n"
     ]
    }
   ],
   "source": [
    "#Train data\n",
    "dataPath = os.path.join('data', 'train')\n",
    "train_data = None\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if train_data is None:\n",
    "            train_data = data\n",
    "        else:\n",
    "            train_data = np.concatenate((train_data, data))\n",
    "\n",
    "y_train = None\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.gs\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if y_train is None:\n",
    "            y_train = data\n",
    "        else:\n",
    "            y_train = np.concatenate((y_train, data))\n",
    "\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "##Test data\n",
    "dataPath = os.path.join('data', 'test-gold')\n",
    "test_data = None\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if test_data is None:\n",
    "            test_data = data\n",
    "        else:\n",
    "            test_data = np.concatenate((test_data, data))\n",
    "\n",
    "y_test = None\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.gs\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if y_test is None:\n",
    "            y_test = data\n",
    "        else:\n",
    "            y_test = np.concatenate((y_test, data))\n",
    "\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation of training data similarities\n",
      "<###################################################################################################>\r\n",
      "Finished computation of training data similarities\n",
      "\n",
      "Starting computation of testing data similarities\n",
      "<###################################################################################################>\r\n",
      "Finished computation of testing data similarities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "INF = np.finfo(float).max / 1e200\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print('Starting computation of training data similarities')\n",
    "train_features = compute_similarity(train_data)\n",
    "train_features[train_features == np.inf] = INF\n",
    "x_train = scaler.fit_transform(train_features)\n",
    "print('Finished computation of training data similarities\\n')\n",
    "\n",
    "print('Starting computation of testing data similarities')\n",
    "test_features = compute_similarity(test_data)\n",
    "test_features[test_features == np.inf] = INF\n",
    "x_test = scaler.fit_transform(test_features)\n",
    "print('Finished computation of testing data similarities\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Random Forest Regressor: 0.5794861352576831\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators=6, random_state=42)\n",
    "# Train the model on training data\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(x_test)\n",
    "# Calculate the absolute errors\n",
    "rf_correlation = pearsonr(predictions, y_test)[0]\n",
    "print(f'Pearson correlation using Random Forest Regressor: {rf_correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Support Vector Regressor: 0.583534523746479\n"
     ]
    }
   ],
   "source": [
    "regr = SVR()\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "svr_pred = regr.predict(x_test)\n",
    "svr_correlation = pearsonr(svr_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Support Vector Regressor: {svr_correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Support Vector Regressor: 0.5674670208589774\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "knn_pred = model.predict(x_test)\n",
    "\n",
    "knn_correlation = pearsonr(knn_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Support Vector Regressor: {knn_correlation}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}