{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jMkKylwoeys"
   },
   "source": [
    "# IHLT final project - Semantic Textual Similarity\n",
    "Nikita Belooussov and Santiago del Rey JuÃ¡rez\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\nbelo\\anaconda3\\envs\\ihlt2\\lib\\site-packages (0.0.58)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\nbelo\\anaconda3\\envs\\ihlt2\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in c:\\users\\nbelo\\anaconda3\\envs\\ihlt2\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\nbelo\\anaconda3\\envs\\ihlt2\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: num2words in c:\\users\\nbelo\\anaconda3\\envs\\ihlt2\\lib\\site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\nbelo\\anaconda3\\envs\\ihlt2\\lib\\site-packages (from num2words) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "# requires visual studios builder from https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "import pickle\n",
    "!pip install contractions\n",
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nv-sb04fXE1",
    "outputId": "a35cc345-8843-4a63-f109-6db1d60463c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nbelo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import string\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus.reader import WordNetError\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import num2words\n",
    "import random\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "contractions.add('U.S.', 'United States')\n",
    "contractions.add('U.S.A', 'United States of America')\n",
    "contractions.add('E.U.', 'European Union')\n",
    "\n",
    "#if this does not work run python -m spacy download en in terminal and restart the program running the code\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def expand_contractions(s0, s1):\n",
    "    s0 = contractions.fix(s0)\n",
    "    s1 = contractions.fix(s1)\n",
    "    return s0, s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change numbers to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeNums(s0):\n",
    "      s0=s0.split()\n",
    "      new_s0=[]\n",
    "      for i in s0:\n",
    "            if i.isdigit():\n",
    "                  new_s0.append(num2words.num2words(i))\n",
    "            else:\n",
    "                  new_s0.append(i)\n",
    "      s0 = ' '.join(new_s0)\n",
    "      return s0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.lower() for w in nltk.word_tokenize(sentence) if\n",
    "            not all(c in punct for c in w) and w.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def name_entity_tokenization(sentence):\n",
    "    doc = nlp(sentence.lower())\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        tokens = [token for token in doc]\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(doc[ent.start:ent.end],\n",
    "                              attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
    "    s0_ne = [token.text for token in doc]\n",
    "    return s0_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(pair):\n",
    "    if pair[1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "        if pair[1][\n",
    "            0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "            return wnl.lemmatize(pair[0].lower(), pos=wordnet.ADJ)\n",
    "        return wnl.lemmatize(pair[0].lower(), pos=pair[1][0].lower())\n",
    "    return pair[0]\n",
    "\n",
    "\n",
    "def lemmatize_sentence(words):\n",
    "    pairs = nltk.pos_tag(words)\n",
    "    lemmas = [lemmatize(pair) for pair in pairs]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Synset similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_similarity(s0, s1, method, ic):\n",
    "    if s0 is not None and s1 is not None:\n",
    "        if method == 'path':\n",
    "            return s0.path_similarity(s1)\n",
    "        elif method == 'wup':\n",
    "            return s0.wup_similarity(s1)\n",
    "        elif s0.pos() == s1.pos():\n",
    "            if method == \"lch\":\n",
    "                return s0.lch_similarity(s1)\n",
    "            elif ic is None:\n",
    "                raise ValueError(\"ic parameter is missing\")\n",
    "            elif method == \"lin\":\n",
    "                try:\n",
    "                    return s0.lin_similarity(s1, ic)\n",
    "                except WordNetError:\n",
    "                    return None\n",
    "            elif method == 'res':\n",
    "                try:\n",
    "                    return s0.res_similarity(s1, ic)\n",
    "                except WordNetError:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Dictionary used to store already computed synsets\n",
    "try:\n",
    "    with open('synset_dic.pkl', 'rb') as file:\n",
    "        computed_synsets = pickle.load(file)\n",
    "except IOError:\n",
    "    computed_synsets = {}\n",
    "\n",
    "\n",
    "def max_similarity(s0, s1, method, ic):\n",
    "    if s0 == s1:\n",
    "        return 1\n",
    "\n",
    "    if (s0, s1, method) in computed_synsets:\n",
    "        return computed_synsets[(s0, s1, method)]\n",
    "\n",
    "    synsets0 = wordnet.synsets(s0)\n",
    "    synsets1 = wordnet.synsets(s1)\n",
    "\n",
    "    similarities = []\n",
    "    for syn0 in synsets0:\n",
    "        for syn1 in synsets1:\n",
    "            similarity = get_wordnet_similarity(syn0, syn1, method, ic)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    if len(similarities) > 0:\n",
    "        max_sim = max(similarities)\n",
    "        computed_synsets[(s0, s1, method)] = max_sim\n",
    "        return max_sim\n",
    "    else:\n",
    "        computed_synsets[(s0, s1, method)] = 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mean_simimilarity(lemmas0, lemmas1, method, ic):\n",
    "    similarity_sum = 0\n",
    "    for l0 in lemmas0:\n",
    "        similarity_sum += max([max_similarity(l0, l1, method, ic) for l1 in lemmas1])\n",
    "    return similarity_sum / len(lemmas0)\n",
    "\n",
    "\n",
    "def synset_similarity(lemmas0, lemmas1, method, ic=None):\n",
    "    mean_sim0 = mean_simimilarity(lemmas0, lemmas1, method, ic)\n",
    "    mean_sim1 = mean_simimilarity(lemmas1, lemmas0, method, ic)\n",
    "\n",
    "    if mean_sim0 > 0 or mean_sim1 > 0:\n",
    "        return mean_sim0 + mean_sim1 / 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lesk similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lesk_similarity(words0, words1):\n",
    "    w0_pos = nltk.pos_tag(words0)\n",
    "    w1_pos = nltk.pos_tag(words1)\n",
    "\n",
    "    s0_lesk = []\n",
    "    for i in range(len(w0_pos)):\n",
    "        if w0_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w0_pos[i][1][\n",
    "                0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s0_lesk.append(nltk.wsd.lesk(words0, w0_pos[i][0], pos=w0_pos[i][1][0].lower()))\n",
    "\n",
    "    s1_lesk = []\n",
    "    for i in range(len(w1_pos)):\n",
    "        if w1_pos[i][1][0] in {'N', 'V', 'J', 'R'}:  #N- noun, V- verb, J- adjective, R-adverb\n",
    "            if w1_pos[i][1][\n",
    "                0] == 'J':  #this is used due to wordnet using a different label for adjectives than one given by nltk\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=wordnet.ADJ))\n",
    "            else:\n",
    "                s1_lesk.append(nltk.wsd.lesk(words1, w1_pos[i][0], pos=w1_pos[i][1][0].lower()))\n",
    "\n",
    "    return 1 - jaccard_distance(set(s0_lesk), set(s1_lesk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(words0, words1):\n",
    "    return 1 - jaccard_distance(set(words0), set(words1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninputTexts = []\\ndataPath = os.path.join(\\'data\\', \\'train\\')\\nx = []\\nfor filename in os.listdir(dataPath):\\n    if \"STS.input\" in filename:\\n        print(os.path.join(dataPath, filename))\\n        file = os.path.join(dataPath, filename)\\n        text = pd.read_csv(file, sep=\\'\\t\\', lineterminator=\\'\\n\\', names=[\\'sentence0\\', \\'sentence1\\'], header=None,\\n                           quoting=csv.QUOTE_NONE)\\n        for i in range(len(text[\\'sentence0\\'])):\\n            metric = calc_syn(text[\\'sentence0\\'][i], text[\\'sentence1\\'][i])\\n            x.append(metric)\\n        inputTexts.append(text)\\n#print (\"avg:\")\\nnumSen = len(x)\\nx = np.array(x)\\n#print(sum(x[:, 0]) / numSen)\\n#print(sum(x[:, 1]) / numSen)\\n#print(sum(x[:, 2]) / numSen)\\n#print(sum(x[:, 3]) / numSen)\\n#print(sum(x[:, 4]) / numSen)\\n#print(sum(x[:, 5]) / numSen)'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_syn(s0,s1):\n",
    "    if len(s1)<len(s0):\n",
    "        s0,s1=s1,s0\n",
    "    #s0, s1 = expand_contractions(s0, s1)\n",
    "    #s0 = tokenize(s0)\n",
    "    #s1 = tokenize(s1)\n",
    "    #s0 = lemmatize_sentence(s0)\n",
    "    #s1 = lemmatize_sentence(s1)\n",
    "\n",
    "    synonyms1=[]\n",
    "    synonyms2=[]\n",
    "    for i in s0:\n",
    "        synonyms1 = [*synonyms1, *wordnet.synsets(i)] \n",
    "    for i in s1:\n",
    "        synonyms2 = [*synonyms2, *wordnet.synsets(i)] \n",
    "\n",
    "    count=0\n",
    "    for i in synonyms1:\n",
    "        if i in synonyms2:\n",
    "            count=count+1\n",
    "    if (len(synonyms1) != 0) and (len(synonyms2) != 0):\n",
    "        return count/len(synonyms1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "#Test\n",
    "\"\"\"\n",
    "inputTexts = []\n",
    "dataPath = os.path.join('data', 'train')\n",
    "x = []\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        file = os.path.join(dataPath, filename)\n",
    "        text = pd.read_csv(file, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None,\n",
    "                           quoting=csv.QUOTE_NONE)\n",
    "        for i in range(len(text['sentence0'])):\n",
    "            metric = calc_syn(text['sentence0'][i], text['sentence1'][i])\n",
    "            x.append(metric)\n",
    "        inputTexts.append(text)\n",
    "#print (\"avg:\")\n",
    "numSen = len(x)\n",
    "x = np.array(x)\n",
    "#print(sum(x[:, 0]) / numSen)\n",
    "#print(sum(x[:, 1]) / numSen)\n",
    "#print(sum(x[:, 2]) / numSen)\n",
    "#print(sum(x[:, 3]) / numSen)\n",
    "#print(sum(x[:, 4]) / numSen)\n",
    "#print(sum(x[:, 5]) / numSen)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TF IDF and cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tf_similarity(s0, s1):\n",
    "    # Generate the tf-idf vectors for the corpus\n",
    "    words0 = ' '.join([str(elem) for elem in s0])\n",
    "    words1 = ' '.join([str(elem) for elem in s1])\n",
    "\n",
    "    tfvec = TfidfVectorizer()\n",
    "    tfidf_matrix = tfvec.fit_transform([words0, words1])\n",
    "\n",
    "    return cosine_similarity(tfidf_matrix, tfidf_matrix)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### N-Gram similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninputTexts = []\\ndataPath = os.path.join(\\'data\\', \\'train\\')\\nx = []\\nfor filename in os.listdir(dataPath):\\n    if \"STS.input\" in filename:\\n        print(os.path.join(dataPath, filename))\\n        file = os.path.join(dataPath, filename)\\n        text = pd.read_csv(file, sep=\\'\\t\\', lineterminator=\\'\\n\\', names=[\\'sentence0\\', \\'sentence1\\'], header=None,\\n                           quoting=csv.QUOTE_NONE)\\n        for i in range(len(text[\\'sentence0\\'])):\\n            metric = compute_n_grams(text[\\'sentence0\\'][i], text[\\'sentence1\\'][i])\\n            x.append(metric)\\n        inputTexts.append(text)\\n#print (\"avg:\")\\nnumSen = len(x)\\nx = np.array(x)\\nprint(sum(x[:, 0]) / numSen)\\nprint(sum(x[:, 1]) / numSen)\\nprint(sum(x[:, 2]) / numSen)\\nprint(sum(x[:, 3]) / numSen)\\nprint(sum(x[:, 4]) / numSen)\\nprint(sum(x[:, 5]) / numSen)'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_n_grams(s0, s1):\n",
    "    #words1=[word for word in s0.split(\" \") if word not in set(stopwords.words('english'))]\n",
    "    low_size = 5\n",
    "    if len(s1) < len(s0):\n",
    "        s0, s1 = s1, s0\n",
    "    if len(s0) < 5:\n",
    "        low_size = len(s0)\n",
    "    #s0, s1 = expand_contractions(s0, s1)\n",
    "    #s0 = tokenize(s0)\n",
    "    #s1 = tokenize(s1)\n",
    "    #s0 = lemmatize_sentence(s0)\n",
    "    #s1 = lemmatize_sentence(s1)\n",
    "    #print (s0)\n",
    "    #print (s1)\n",
    "    #print(\"Sentence after removing stopwords:\",s0)\n",
    "    metrics = [0, 0, 0, 0, 0, 0]\n",
    "    for i in range(2, low_size):\n",
    "        n_grams1 = zip(*[s0[k:] for k in range(0, i)])\n",
    "        n_grams2 = zip(*[s1[k:] for k in range(0, i)])\n",
    "        count = 0\n",
    "\n",
    "        n_grams1 = [' '.join(ngram) for ngram in n_grams1]\n",
    "        n_grams2 = [' '.join(ngram) for ngram in n_grams2]\n",
    "        #print (set(nGrams2))\n",
    "        #print (set(nGrams1))\n",
    "        for j in set(n_grams1):\n",
    "            if j in set(n_grams2):\n",
    "                count += 1\n",
    "        if (len(n_grams1) != 0) and (len(n_grams2) != 0):\n",
    "            #print(1-jaccard_distance(set(nGrams1), set(nGrams2)))\n",
    "            #print (count)\n",
    "            metrics[(i - 2) * 2] = count / len(set(n_grams1))\n",
    "            metrics[((i - 2) * 2) + 1] = 1 - jaccard_distance(set(n_grams1), set(n_grams2))\n",
    "    return metrics\n",
    "\n",
    "\"\"\"\n",
    "inputTexts = []\n",
    "dataPath = os.path.join('data', 'train')\n",
    "x = []\n",
    "for filename in os.listdir(dataPath):\n",
    "    if \"STS.input\" in filename:\n",
    "        print(os.path.join(dataPath, filename))\n",
    "        file = os.path.join(dataPath, filename)\n",
    "        text = pd.read_csv(file, sep='\\t', lineterminator='\\n', names=['sentence0', 'sentence1'], header=None,\n",
    "                           quoting=csv.QUOTE_NONE)\n",
    "        for i in range(len(text['sentence0'])):\n",
    "            metric = compute_n_grams(text['sentence0'][i], text['sentence1'][i])\n",
    "            x.append(metric)\n",
    "        inputTexts.append(text)\n",
    "#print (\"avg:\")\n",
    "numSen = len(x)\n",
    "x = np.array(x)\n",
    "print(sum(x[:, 0]) / numSen)\n",
    "print(sum(x[:, 1]) / numSen)\n",
    "print(sum(x[:, 2]) / numSen)\n",
    "print(sum(x[:, 3]) / numSen)\n",
    "print(sum(x[:, 4]) / numSen)\n",
    "print(sum(x[:, 5]) / numSen)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_SYMBOLS = 50\n",
    "\n",
    "\n",
    "def compute_similarity(x):\n",
    "    features = []\n",
    "    n_samples = x.shape[0]\n",
    "    perc = round(0.02 * n_samples)\n",
    "    counter = 0\n",
    "    progress = 0\n",
    "    for sentence_0, sentence_1 in x:\n",
    "        sentence_0=changeNums(sentence_0)\n",
    "        sentence_1=changeNums(sentence_1)\n",
    "        sentence_0, sentence_1 = expand_contractions(sentence_0, sentence_1)\n",
    "        words0 = tokenize(sentence_0)\n",
    "        words1 = tokenize(sentence_1)\n",
    "        s0_lemmas = lemmatize_sentence(words0)\n",
    "        s1_lemmas = lemmatize_sentence(words1)\n",
    "        s0_ne = name_entity_tokenization(sentence_0)\n",
    "        s1_ne = name_entity_tokenization(sentence_1)\n",
    "        n_grams_results=compute_n_grams(s0_lemmas, s1_lemmas)\n",
    "        features.append([\n",
    "            jaccard_similarity(words0, words1),\n",
    "            jaccard_similarity(s0_lemmas, s1_lemmas),\n",
    "            jaccard_similarity(s0_ne, s1_ne),\n",
    "            tf_similarity(words0, words1),\n",
    "            tf_similarity(s0_lemmas, s1_lemmas),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'path'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lch'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'wup'),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lin', brown_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'lin', semcor_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'res', brown_ic),\n",
    "            synset_similarity(s0_lemmas, s1_lemmas, 'res', semcor_ic),\n",
    "            lesk_similarity(words0, words1),\n",
    "            n_grams_results[0],\n",
    "            n_grams_results[1],\n",
    "            n_grams_results[2],\n",
    "            n_grams_results[3],\n",
    "            n_grams_results[4],\n",
    "            n_grams_results[5],\n",
    "            calc_syn(s0_lemmas,s1_lemmas)\n",
    "        ])\n",
    "\n",
    "        progress = print_progress(counter, perc, progress)\n",
    "        counter += 1\n",
    "\n",
    "    print()\n",
    "    return np.array(features, dtype=np.float64)\n",
    "\n",
    "\n",
    "def print_progress(counter, perc, progress):\n",
    "    if (counter % perc) == 0:\n",
    "        print('<' + '#' * progress + '.' * (N_SYMBOLS - progress) + '>', end='\\r')\n",
    "        return progress + 1\n",
    "    return progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', lineterminator='\\n', header=None,\n",
    "                       quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Train data\n",
    "dataPath = os.path.join('data', 'train')\n",
    "train_data = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.input\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if train_data is None:\n",
    "            train_data = data\n",
    "        else:\n",
    "            train_data = np.concatenate((train_data, data))\n",
    "\n",
    "y_train = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.gs\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if y_train is None:\n",
    "            y_train = data\n",
    "        else:\n",
    "            y_train = np.concatenate((y_train, data))\n",
    "\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "##Test data\n",
    "dataPath = os.path.join('data', 'test-gold')\n",
    "test_data = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.input\" in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if test_data is None:\n",
    "            test_data = data\n",
    "        else:\n",
    "            test_data = np.concatenate((test_data, data))\n",
    "\n",
    "y_test = None\n",
    "for filename in sorted(os.listdir(dataPath)):\n",
    "    if \"STS.gs\" in filename and \"ALL\" not in filename:\n",
    "        data = read_file(os.path.join(dataPath, filename)).to_numpy()\n",
    "        if y_test is None:\n",
    "            y_test = data\n",
    "        else:\n",
    "            y_test = np.concatenate((y_test, data))\n",
    "\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation of training data similarities\n",
      "\n",
      "Finished computation of training data similarities\n",
      "\n",
      "Starting computation of testing data similarities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nbelo\\anaconda3\\envs\\IHLT2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1544: RuntimeWarning: overflow encountered in multiply\n",
      "  sqr = np.multiply(arr, arr, out=arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<##################################################>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nbelo\\anaconda3\\envs\\IHLT2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1544: RuntimeWarning: overflow encountered in multiply\n",
      "  sqr = np.multiply(arr, arr, out=arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished computation of testing data similarities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "INF = np.finfo(float).max / 1e200\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print('Starting computation of training data similarities')\n",
    "train_features = compute_similarity(train_data)\n",
    "train_features[train_features == np.inf] = INF\n",
    "x_train = scaler.fit_transform(train_features)\n",
    "print('Finished computation of training data similarities\\n')\n",
    "\n",
    "print('Starting computation of testing data similarities')\n",
    "test_features = compute_similarity(test_data)\n",
    "test_features[test_features == np.inf] = INF\n",
    "x_test = scaler.fit_transform(test_features)\n",
    "print('Finished computation of testing data similarities\\n')\n",
    "\n",
    "with open('synset_dic.pkl', 'wb') as file:\n",
    "    pickle.dump(computed_synsets, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selected_features = [0, # jaccard using words\n",
    "                     1, # jaccard using lemmas\n",
    "                     2, # jaccard using NEs\n",
    "                     3, # tf similarity using words\n",
    "                     4, # tf similairty using lemmas\n",
    "                     5, # path similarity\n",
    "                     6, # lch similarity\n",
    "                     7, # wup similarity\n",
    "                     #  8, # lin brown similarity\n",
    "                     9, # lin semcor similarity\n",
    "                     #  10, # res brown similarity\n",
    "                     #  11, # res semcor similarity\n",
    "                     #  12 # lesk similarity\n",
    "                     13, #n-Grams 0\n",
    "                     14, #n-Grams 1\n",
    "                     15, #n-grams 2\n",
    "                     16, #ngrams 3\n",
    "                     17, #ngrams 4\n",
    "                     18, #ngrams 5\n",
    "                     #19 #synnonyms\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Random Forest Regressor: 0.6012951661515349\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators=24, random_state=70)\n",
    "# Train the model on training data\n",
    "rf.fit(x_train[:, selected_features], y_train)\n",
    "\n",
    "# Use the forest's predict method on the test data  \n",
    "rf_pred = rf.predict(x_test[:, selected_features])\n",
    "# Calculate the absolute errors \n",
    "rf_correlation = pearsonr(rf_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Random Forest Regressor: {rf_correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Support Vector Regressor: 0.7045320849729222\n"
     ]
    }
   ],
   "source": [
    "regr = SVR()\n",
    "regr.fit(x_train[:, selected_features], y_train)\n",
    "svr_pred = regr.predict(x_test[:, selected_features])\n",
    "svr_correlation = pearsonr(svr_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Support Vector Regressor: {svr_correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Support Vector Regressor: 0.6972592984735935\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsRegressor(n_neighbors=26)\n",
    "\n",
    "model.fit(x_train[:, selected_features], y_train)\n",
    "knn_pred = model.predict(x_test[:, selected_features])\n",
    "knn_correlation = pearsonr(knn_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Support Vector Regressor: {knn_correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Support Vector Regressor: 0.7286156724793483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(22,14,45),activation=\"relu\" ,random_state=1, max_iter=2000).fit(x_train[:, selected_features], y_train)\n",
    "nn_pred=model.predict(x_test[:, selected_features])\n",
    "nn_correlation = pearsonr(nn_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Support Vector Regressor: {nn_correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Support Vector Regressor: 0.6963798798824969\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "rng = np.random.RandomState(1)\n",
    "# Fit regression model\n",
    "lassoReg = linear_model.Lasso(alpha= .005)\n",
    "lassoReg.fit(x_train[:,selected_features],y_train)\n",
    "lass_pred=lassoReg.predict(x_test[:,selected_features])\n",
    "lass_correlation = pearsonr(lass_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Support Vector Regressor: {lass_correlation}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation using Support Vector Regressor: 0.7344720629431842\n"
     ]
    }
   ],
   "source": [
    "avg_pred=lass_pred+nn_pred+knn_pred\n",
    "\n",
    "avg_correlation = pearsonr(avg_pred, y_test)[0]\n",
    "print(f'Pearson correlation using Support Vector Regressor: {avg_correlation}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab05-Santiago-del-Rey-Juarez_Nikita-Belooussov.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "56b22a985f18d44d50171c67439d384ccb9cd51faf15dc17dcf099ae374960d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
